#!/usr/bin/env python3
"""
train_cnn.py

Training script for the MAD CNN on ESRF cluster.

This script:
1. Loads training data from .npz files generated by generate_training_data.py
2. Creates PyTorch DataLoader with online augmentation
3. Trains the MADNet model with physics-informed loss
4. Saves checkpoints and logs training progress
5. Supports resuming from checkpoints

Usage:
------
    # Basic training
    python train_cnn.py --data-dir /path/to/synthetic_data/test1
    
    # With custom parameters
    python train_cnn.py --data-dir /path/to/data --epochs 200 --batch-size 64
    
    # Resume from checkpoint
    python train_cnn.py --data-dir /path/to/data --resume checkpoint_best.pt

SLURM submission:
-----------------
    #!/bin/bash
    #SBATCH --job-name=nanomad_train
    #SBATCH --gres=gpu:1
    #SBATCH --cpus-per-task=8
    #SBATCH --mem=32G
    #SBATCH --time=12:00:00
    #SBATCH --output=train_%j.log
    
    module load conda
    conda activate nanomad
    
    python train_cnn.py --data-dir /path/to/synthetic_data/test1 --epochs 200

Author: Claude (Anthropic) + Thomas
Date: December 2024
"""

import os
import sys
import json
import time
import argparse
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Tuple, Optional

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, random_split

# Local imports - use try/except for flexible import paths
try:
    # When running from repository root: python src/train.py
    from src.mad_model import MADNet, create_model
    from src.mad_loss import MADLoss
    from src.augmentation import augment_sample
except ImportError:
    # When running from src directory: python train.py
    from mad_model import MADNet, create_model
    from mad_loss import MADLoss
    from augmentation import augment_sample


# =============================================================================
# CONFIGURATION DEFAULTS
# =============================================================================

DEFAULT_CONFIG = {
    # Data (default, override with --data-dir)
    'data_dir': 'synthetic_data',
    'val_split': 0.1,           # 10% for validation
    
    # Model
    'n_energies': 8,
    'base_channels': 32,        # 32 → ~200K params
    
    # Training
    'batch_size': 64,
    'epochs': 200,
    'learning_rate': 1e-3,
    'weight_decay': 1e-4,
    
    # Loss weights
    'lambda_unit': 0.1,         # Unit circle constraint weight
    
    # RECONSTRUCTION LOSS DISABLED (lambda_recon = 0.0)
    # -------------------------------------------------
    # The reconstruction loss converts log-scale predictions back to linear scale
    # using expm1(). With magnitudes up to 80,000, even small errors in log space
    # (e.g., predicting 15 instead of 11) become enormous in linear space 
    # (exp(15) ≈ 3 million), causing loss explosions into the trillions.
    # 
    # The MSE loss already compares predictions to physics-derived ground truth,
    # so we still learn correct physics. Disabling recon loss gives stable training.
    #
    # Previous attempts:
    #   - lambda_recon=0.1: Training loss exploded (10^21), val MSE ~20 million
    #   - lambda_recon=0.1 + log-scale: Val MSE ~8-9, but train loss still chaotic
    #   - lambda_recon=0.0: Stable training (current setting)
    'lambda_recon': 0.0,        # Intensity reconstruction weight (disabled)

    # F_A-SPECIFIC LOSS (Fix F_A holes)
    # ---------------------------------------------
    # Adds an EXTRA loss term for F_A channel with UNIFORM weighting.
    # This forces the network to learn F_A in ALL regions, not just bright ones.
    # The standard intensity-weighted MSE ignores low-intensity regions where
    # F_A may be small but non-zero, causing "holes" in F_A predictions.
    #
    # Recommended starting value: 1.0 (equivalent weight to MSE loss)
    # Set to 0.0 to disable (backward compatible with previous training)
    'lambda_fa': 0.0,           # F_A emphasis weight (0.0 = disabled, 1.0 = recommended)

    # Augmentation
    'augment': True,
    'aug_intensity_scale': (0.5, 2.0),
    'aug_noise_prob': 0.5,
    'aug_noise_range': (0.01, 0.1),

    # Input preprocessing
    # Log-transform input intensities to compress dynamic range
    # This helps the CNN learn from both bright and dim regions equally
    'log_transform_intensity': False,  # Set to True for balanced training

    # Loss weighting scheme (see mad_loss.py for details)
    # Options: 'sqrt' (original), 'log' (recommended), 'uniform', 'sqrt_floor'
    'weight_scheme': 'sqrt',  # Use 'log' for balanced training
    
    # Learning rate schedule
    'lr_scheduler': 'cosine',   # 'cosine' or 'step'
    'lr_warmup_epochs': 5,
    
    # Checkpointing
    'save_every': 10,           # Save checkpoint every N epochs
    'output_dir': './training_output',
    
    # Misc
    'num_workers': 4,
    'seed': 42,
}


# =============================================================================
# DATASET
# =============================================================================

class MADDataset(Dataset):
    """
    PyTorch Dataset for MAD training data.

    Loads pre-generated .npz files and applies optional augmentation.

    Data format in .npz files:
        X: (n_patches, 16, 16, 8) — intensity patches
        Y: (n_patches, 16, 16, 4) — label patches
        f_prime: (8,) — f'(E) for this particle
        f_double_prime: (8,) — f''(E) for this particle

    PyTorch convention is channels-first, so we transpose:
        X: (n_patches, 8, 16, 16)
        Y: (n_patches, 4, 16, 16)
    """

    def __init__(
        self,
        data_dir: str,
        augment: bool = True,
        augment_config: Dict = None,
        log_transform_intensity: bool = False,
        verbose: bool = True
    ):
        """
        Parameters
        ----------
        data_dir : str
            Directory containing particle_XXXX.npz files
        augment : bool
            Whether to apply data augmentation
        augment_config : dict
            Configuration for augmentation (see data_augmentation.py)
        log_transform_intensity : bool
            If True, apply log1p transform to input intensities.
            This compresses the dynamic range from [0, 10^6] to [0, ~14],
            helping the CNN learn equally from bright and dim regions.
            IMPORTANT: If True, inference must also apply log1p to inputs!
        verbose : bool
            Print loading progress
        """
        self.data_dir = Path(data_dir)
        self.augment = augment
        self.augment_config = augment_config or {}
        self.log_transform_intensity = log_transform_intensity
        
        # Find all particle files
        self.particle_files = sorted(self.data_dir.glob('particle_*.npz'))
        
        if len(self.particle_files) == 0:
            raise ValueError(f"No particle files found in {data_dir}")
        
        if verbose:
            print(f"Found {len(self.particle_files)} particle files in {data_dir}")
        
        # Load all data into memory
        # This is feasible because our dataset is relatively small (~1-2 GB)
        self._load_all_data(verbose)
    
    def _load_all_data(self, verbose: bool):
        """Load all particle data into memory."""
        
        X_list = []
        Y_list = []
        fp_list = []
        fs_list = []
        particle_idx_list = []
        
        for i, filepath in enumerate(self.particle_files):
            data = np.load(filepath, allow_pickle=True)
            
            X = data['X']  # (n_patches, 16, 16, 8)
            Y = data['Y']  # (n_patches, 16, 16, 4)
            fp = data['f_prime']  # (8,)
            fs = data['f_double_prime']  # (8,)
            
            n_patches = X.shape[0]
            
            X_list.append(X)
            Y_list.append(Y)
            fp_list.append(np.tile(fp, (n_patches, 1)))  # Repeat for each patch
            fs_list.append(np.tile(fs, (n_patches, 1)))  # Repeat for each patch
            particle_idx_list.append(np.full(n_patches, i))
            
            if verbose and (i + 1) % 50 == 0:
                print(f"  Loaded {i + 1}/{len(self.particle_files)} particles...")
        
        # Concatenate all data
        self.X = np.concatenate(X_list, axis=0)  # (N, 16, 16, 8)
        self.Y = np.concatenate(Y_list, axis=0)  # (N, 16, 16, 4)
        self.f_prime = np.concatenate(fp_list, axis=0)  # (N, 8)
        self.f_double_prime = np.concatenate(fs_list, axis=0)  # (N, 8)
        self.particle_idx = np.concatenate(particle_idx_list, axis=0)  # (N,)
        
        if verbose:
            print(f"  Total patches: {len(self.X)}")
            print(f"  X shape: {self.X.shape}")
            print(f"  Y shape: {self.Y.shape}")
            print(f"  Memory: ~{(self.X.nbytes + self.Y.nbytes) / 1e9:.2f} GB")
    
    def __len__(self):
        return len(self.X)
    
    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:
        """
        Get a single sample.
        
        Returns dict with:
            - intensity: [8, 16, 16] (channels first for PyTorch)
            - labels: [4, 16, 16] (magnitudes are LOG-SCALED)
            - f_prime: [8]
            - f_double_prime: [8]
        
        NOTE: Label channels 0,1 (|F_T|, |F_A|) are log1p transformed!
              At inference, use expm1() to convert predictions back.
        """
        # Get data (still in HWC format)
        intensity = self.X[idx].copy()  # (16, 16, 8)
        labels = self.Y[idx].copy()  # (16, 16, 4)
        fp = self.f_prime[idx].copy()  # (8,)
        fs = self.f_double_prime[idx].copy()  # (8,)
        
        # Apply augmentation (expects HWC format, works on LINEAR scale)
        if self.augment:
            intensity, labels = augment_sample(
                intensity, labels,
                **self.augment_config
            )

        # =====================================================================
        # LOG-TRANSFORM INPUT INTENSITIES (optional)
        # =====================================================================
        # When enabled, compresses intensity dynamic range from [0, 10^6] to [0, ~14]
        # This helps the CNN learn equally from bright (center) and dim (outer) regions.
        # IMPORTANT: If training with this enabled, inference must also apply log1p!
        if self.log_transform_intensity:
            intensity = np.log1p(intensity)

        # =====================================================================
        # LOG-SCALE MAGNITUDE CHANNELS
        # =====================================================================
        # |F_T| and |F_A| can be very large (0 to ~80,000)
        # Transform to log scale for stable training:
        #   log1p(x) = log(1 + x), maps [0, 80000] -> [0, ~11.3]
        # 
        # IMPORTANT: At inference, use expm1() to convert back!
        #   expm1(x) = exp(x) - 1, the inverse of log1p
        # =====================================================================
        labels[..., 0] = np.log1p(labels[..., 0])  # |F_T| -> log(1 + |F_T|)
        labels[..., 1] = np.log1p(labels[..., 1])  # |F_A| -> log(1 + |F_A|)
        # Channels 2, 3 (sin(Δφ), cos(Δφ)) are already in [-1, 1], no transform
        
        # Convert to channels-first (HWC → CHW) for PyTorch
        intensity = np.transpose(intensity, (2, 0, 1))  # (8, 16, 16)
        labels = np.transpose(labels, (2, 0, 1))  # (4, 16, 16)
        
        # Convert to tensors
        return {
            'intensity': torch.from_numpy(intensity).float(),
            'labels': torch.from_numpy(labels).float(),
            'f_prime': torch.from_numpy(fp).float(),
            'f_double_prime': torch.from_numpy(fs).float(),
        }


# =============================================================================
# TRAINING FUNCTIONS
# =============================================================================

def train_one_epoch(
    model: nn.Module,
    dataloader: DataLoader,
    criterion: MADLoss,
    optimizer: optim.Optimizer,
    device: str,
    epoch: int
) -> Dict[str, float]:
    """
    Train for one epoch.
    
    Returns dict with average losses.
    """
    model.train()
    
    total_loss = 0.0
    total_mse = 0.0
    total_unit = 0.0
    total_recon = 0.0
    total_fa = 0.0
    n_batches = 0

    for batch in dataloader:
        # Move to device
        intensity = batch['intensity'].to(device)
        labels = batch['labels'].to(device)
        f_prime = batch['f_prime'].to(device)
        f_double_prime = batch['f_double_prime'].to(device)

        # Forward pass
        optimizer.zero_grad()
        output = model(intensity, f_prime, f_double_prime)

        # Compute loss
        # Note: loss expects channels-last, so we transpose
        output_hwc = output.permute(0, 2, 3, 1)  # [B, H, W, 4]
        labels_hwc = labels.permute(0, 2, 3, 1)  # [B, H, W, 4]
        intensity_hwc = intensity.permute(0, 2, 3, 1)  # [B, H, W, 8]

        loss, loss_dict = criterion(
            pred=output_hwc,
            target=labels_hwc,
            intensities=intensity_hwc,
            f_prime=f_prime,
            f_double_prime=f_double_prime
        )

        # Backward pass
        loss.backward()

        # Gradient clipping
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

        optimizer.step()

        # Accumulate stats
        total_loss += loss_dict['total']
        total_mse += loss_dict['mse']
        total_unit += loss_dict['unit']
        total_recon += loss_dict['recon']
        total_fa += loss_dict.get('fa', 0.0)
        n_batches += 1

    return {
        'loss': total_loss / n_batches,
        'mse': total_mse / n_batches,
        'unit': total_unit / n_batches,
        'recon': total_recon / n_batches,
        'fa': total_fa / n_batches,
    }


@torch.no_grad()
def validate(
    model: nn.Module,
    dataloader: DataLoader,
    criterion: MADLoss,
    device: str
) -> Dict[str, float]:
    """
    Validate the model.
    
    Returns dict with average losses.
    """
    model.eval()

    total_loss = 0.0
    total_mse = 0.0
    total_unit = 0.0
    total_recon = 0.0
    total_fa = 0.0
    n_batches = 0

    for batch in dataloader:
        # Move to device
        intensity = batch['intensity'].to(device)
        labels = batch['labels'].to(device)
        f_prime = batch['f_prime'].to(device)
        f_double_prime = batch['f_double_prime'].to(device)

        # Forward pass
        output = model(intensity, f_prime, f_double_prime)

        # Compute loss (channels-last format)
        output_hwc = output.permute(0, 2, 3, 1)
        labels_hwc = labels.permute(0, 2, 3, 1)
        intensity_hwc = intensity.permute(0, 2, 3, 1)

        loss, loss_dict = criterion(
            pred=output_hwc,
            target=labels_hwc,
            intensities=intensity_hwc,
            f_prime=f_prime,
            f_double_prime=f_double_prime
        )

        # Accumulate stats
        total_loss += loss_dict['total']
        total_mse += loss_dict['mse']
        total_unit += loss_dict['unit']
        total_recon += loss_dict['recon']
        total_fa += loss_dict.get('fa', 0.0)
        n_batches += 1

    return {
        'loss': total_loss / n_batches,
        'mse': total_mse / n_batches,
        'unit': total_unit / n_batches,
        'recon': total_recon / n_batches,
        'fa': total_fa / n_batches,
    }


# =============================================================================
# CHECKPOINTING
# =============================================================================

def save_checkpoint(
    model: nn.Module,
    optimizer: optim.Optimizer,
    scheduler,
    epoch: int,
    train_losses: List[Dict],
    val_losses: List[Dict],
    config: Dict,
    filepath: str
):
    """Save training checkpoint."""
    checkpoint = {
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'scheduler_state_dict': scheduler.state_dict() if scheduler else None,
        'train_losses': train_losses,
        'val_losses': val_losses,
        'config': config,
    }
    torch.save(checkpoint, filepath)


def load_checkpoint(filepath: str, model: nn.Module, optimizer: optim.Optimizer, scheduler):
    """Load training checkpoint."""
    checkpoint = torch.load(filepath)
    
    model.load_state_dict(checkpoint['model_state_dict'])
    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
    if scheduler and checkpoint['scheduler_state_dict']:
        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
    
    return (
        checkpoint['epoch'],
        checkpoint['train_losses'],
        checkpoint['val_losses'],
        checkpoint['config']
    )


# =============================================================================
# MAIN TRAINING LOOP
# =============================================================================

def train(config: Dict):
    """
    Main training function.
    """
    
    # =========================================================================
    # SETUP
    # =========================================================================
    
    print("\n" + "=" * 70)
    print("NANOMAD CNN TRAINING")
    print("=" * 70)
    
    # Set random seeds
    torch.manual_seed(config['seed'])
    np.random.seed(config['seed'])
    
    # Device
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"\nDevice: {device}")
    if device == 'cuda':
        print(f"  GPU: {torch.cuda.get_device_name(0)}")
        print(f"  Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
    
    # Create output directory
    output_dir = Path(config['output_dir'])
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # Save config
    config_path = output_dir / 'config.json'
    with open(config_path, 'w') as f:
        json.dump(config, f, indent=2, default=str)
    print(f"\nConfig saved to: {config_path}")
    
    # =========================================================================
    # DATA
    # =========================================================================
    
    print("\n" + "-" * 70)
    print("LOADING DATA")
    print("-" * 70)
    
    # Augmentation config
    aug_config = {
        'apply_rotation': True,
        'apply_flip': True,
        'apply_intensity_scale': True,
        'apply_noise': True,
        'intensity_scale_range': config['aug_intensity_scale'],
        'noise_range': config['aug_noise_range'],
        'noise_probability': config['aug_noise_prob'],
    }
    
    # Create dataset
    full_dataset = MADDataset(
        data_dir=config['data_dir'],
        augment=config['augment'],
        augment_config=aug_config,
        log_transform_intensity=config.get('log_transform_intensity', False),
        verbose=True
    )
    
    # Split into train/val
    n_total = len(full_dataset)
    n_val = int(n_total * config['val_split'])
    n_train = n_total - n_val
    
    train_dataset, val_dataset = random_split(
        full_dataset,
        [n_train, n_val],
        generator=torch.Generator().manual_seed(config['seed'])
    )
    
    # Disable augmentation for validation
    # (Note: this is a bit hacky since we split after creating the dataset)
    # For proper implementation, you'd create separate datasets
    
    print(f"\nDataset split:")
    print(f"  Training: {n_train} patches")
    print(f"  Validation: {n_val} patches")
    
    # Create dataloaders
    train_loader = DataLoader(
        train_dataset,
        batch_size=config['batch_size'],
        shuffle=True,
        num_workers=config['num_workers'],
        pin_memory=True if device == 'cuda' else False
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=config['batch_size'],
        shuffle=False,
        num_workers=config['num_workers'],
        pin_memory=True if device == 'cuda' else False
    )
    
    print(f"  Batches per epoch: {len(train_loader)} train, {len(val_loader)} val")
    
    # =========================================================================
    # MODEL
    # =========================================================================
    
    print("\n" + "-" * 70)
    print("CREATING MODEL")
    print("-" * 70)
    
    model = create_model(
        n_energies=config['n_energies'],
        base_channels=config['base_channels'],
        device=device
    )
    
    # =========================================================================
    # LOSS & OPTIMIZER
    # =========================================================================
    
    criterion = MADLoss(
        lambda_unit=config['lambda_unit'],
        lambda_recon=config['lambda_recon'],
        lambda_fa=config.get('lambda_fa', 0.0),  # F_A emphasis
        use_recon_loss=False,  # Disabled — causes instability with log-scaled magnitudes
        log_scale_magnitudes=True,  # Magnitudes are log1p transformed in dataset
        weight_scheme=config.get('weight_scheme', 'sqrt'),  # 'log' recommended for balanced training
    )

    optimizer = optim.AdamW(
        model.parameters(),
        lr=config['learning_rate'],
        weight_decay=config['weight_decay']
    )

    # Learning rate scheduler
    if config['lr_scheduler'] == 'cosine':
        scheduler = optim.lr_scheduler.CosineAnnealingLR(
            optimizer,
            T_max=config['epochs'],
            eta_min=config['learning_rate'] * 0.01
        )
    else:
        scheduler = optim.lr_scheduler.StepLR(
            optimizer,
            step_size=50,
            gamma=0.5
        )

    print(f"\nOptimizer: AdamW (lr={config['learning_rate']}, wd={config['weight_decay']})")
    print(f"Scheduler: {config['lr_scheduler']}")
    lambda_fa_val = config.get('lambda_fa', 0.0)
    print(f"Loss: MSE + {config['lambda_unit']}×Unit + {config['lambda_recon']}×Recon + {lambda_fa_val}×FA")
    
    # =========================================================================
    # RESUME FROM CHECKPOINT
    # =========================================================================
    
    start_epoch = 0
    train_losses = []
    val_losses = []
    best_val_loss = float('inf')
    
    if config.get('resume'):
        print(f"\nResuming from checkpoint: {config['resume']}")
        start_epoch, train_losses, val_losses, _ = load_checkpoint(
            config['resume'], model, optimizer, scheduler
        )
        best_val_loss = min(v['loss'] for v in val_losses) if val_losses else float('inf')
        print(f"  Resuming from epoch {start_epoch}")
        print(f"  Best val loss so far: {best_val_loss:.6f}")
    
    # =========================================================================
    # TRAINING LOOP
    # =========================================================================
    
    print("\n" + "=" * 70)
    print("TRAINING")
    print("=" * 70)
    # Include FA column only if lambda_fa > 0
    if config.get('lambda_fa', 0.0) > 0:
        print(f"\n{'Epoch':>6} | {'Train Loss':>12} | {'Val Loss':>12} | {'MSE':>10} | {'Unit':>10} | {'FA':>10} | {'LR':>10} | {'Time':>8}")
        print("-" * 100)
    else:
        print(f"\n{'Epoch':>6} | {'Train Loss':>12} | {'Val Loss':>12} | {'MSE':>10} | {'Unit':>10} | {'LR':>10} | {'Time':>8}")
        print("-" * 85)
    
    for epoch in range(start_epoch, config['epochs']):
        epoch_start = time.time()
        
        # Train
        train_loss = train_one_epoch(
            model, train_loader, criterion, optimizer, device, epoch
        )
        train_losses.append(train_loss)
        
        # Validate
        val_loss = validate(model, val_loader, criterion, device)
        val_losses.append(val_loss)
        
        # Update scheduler
        scheduler.step()
        current_lr = scheduler.get_last_lr()[0]
        
        # Time
        epoch_time = time.time() - epoch_start
        
        # Print progress
        if config.get('lambda_fa', 0.0) > 0:
            print(f"{epoch+1:>6} | {train_loss['loss']:>12.6f} | {val_loss['loss']:>12.6f} | "
                  f"{val_loss['mse']:>10.6f} | {val_loss['unit']:>10.6f} | "
                  f"{val_loss.get('fa', 0.0):>10.6f} | "
                  f"{current_lr:>10.2e} | {epoch_time:>7.1f}s")
        else:
            print(f"{epoch+1:>6} | {train_loss['loss']:>12.6f} | {val_loss['loss']:>12.6f} | "
                  f"{val_loss['mse']:>10.6f} | {val_loss['unit']:>10.6f} | "
                  f"{current_lr:>10.2e} | {epoch_time:>7.1f}s")
        
        # Save best model
        if val_loss['loss'] < best_val_loss:
            best_val_loss = val_loss['loss']
            save_checkpoint(
                model, optimizer, scheduler, epoch + 1,
                train_losses, val_losses, config,
                output_dir / 'checkpoint_best.pt'
            )
            print(f"       └─ New best model saved!")
        
        # Regular checkpoint
        if (epoch + 1) % config['save_every'] == 0:
            save_checkpoint(
                model, optimizer, scheduler, epoch + 1,
                train_losses, val_losses, config,
                output_dir / f'checkpoint_epoch_{epoch+1:04d}.pt'
            )
    
    # =========================================================================
    # FINAL SAVE
    # =========================================================================
    
    save_checkpoint(
        model, optimizer, scheduler, config['epochs'],
        train_losses, val_losses, config,
        output_dir / 'checkpoint_final.pt'
    )
    
    # Save loss history
    loss_history = {
        'train': train_losses,
        'val': val_losses,
    }
    np.save(output_dir / 'loss_history.npy', loss_history)
    
    print("\n" + "=" * 70)
    print("TRAINING COMPLETE")
    print("=" * 70)
    print(f"\nBest validation loss: {best_val_loss:.6f}")
    print(f"Checkpoints saved to: {output_dir}")
    print(f"  - checkpoint_best.pt (best model)")
    print(f"  - checkpoint_final.pt (final model)")
    print(f"  - loss_history.npy (training curves)")


# =============================================================================
# MAIN
# =============================================================================

if __name__ == '__main__':
    parser = argparse.ArgumentParser(
        description='Train MAD CNN for diffraction parameter prediction',
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    
    # Data
    parser.add_argument('--data-dir', type=str, default=DEFAULT_CONFIG['data_dir'],
                        help='Directory containing training data (.npz files)')
    parser.add_argument('--val-split', type=float, default=DEFAULT_CONFIG['val_split'],
                        help='Fraction of data to use for validation')
    
    # Model
    parser.add_argument('--base-channels', type=int, default=DEFAULT_CONFIG['base_channels'],
                        help='Base channel count (32 → ~200K params)')
    
    # Training
    parser.add_argument('--batch-size', type=int, default=DEFAULT_CONFIG['batch_size'],
                        help='Batch size')
    parser.add_argument('--epochs', type=int, default=DEFAULT_CONFIG['epochs'],
                        help='Number of training epochs')
    parser.add_argument('--lr', type=float, default=DEFAULT_CONFIG['learning_rate'],
                        help='Learning rate')
    parser.add_argument('--weight-decay', type=float, default=DEFAULT_CONFIG['weight_decay'],
                        help='Weight decay for AdamW')
    
    # Loss
    parser.add_argument('--lambda-unit', type=float, default=DEFAULT_CONFIG['lambda_unit'],
                        help='Weight for unit circle constraint')
    parser.add_argument('--lambda-recon', type=float, default=DEFAULT_CONFIG['lambda_recon'],
                        help='Weight for intensity reconstruction loss')
    parser.add_argument('--lambda-fa', type=float, default=DEFAULT_CONFIG['lambda_fa'],
                        help='Weight for F_A-specific loss (uniform weighting). '
                             'Adds extra emphasis on F_A channel to prevent holes. '
                             '0.0=disabled, 1.0=recommended for F_A fix.')
    parser.add_argument('--weight-scheme', type=str, default=DEFAULT_CONFIG['weight_scheme'],
                        choices=['sqrt', 'log', 'uniform', 'sqrt_floor'],
                        help='Loss weighting scheme: sqrt (original), log (recommended), uniform, sqrt_floor')

    # Input preprocessing
    parser.add_argument('--log-transform-intensity', action='store_true',
                        help='Apply log1p to input intensities (recommended for balanced training)')

    # Augmentation
    parser.add_argument('--no-augment', action='store_true',
                        help='Disable data augmentation')
    
    # Output
    parser.add_argument('--output-dir', type=str, default=DEFAULT_CONFIG['output_dir'],
                        help='Directory to save checkpoints and logs')
    parser.add_argument('--save-every', type=int, default=DEFAULT_CONFIG['save_every'],
                        help='Save checkpoint every N epochs')
    
    # Resume
    parser.add_argument('--resume', type=str, default=None,
                        help='Path to checkpoint to resume from')
    
    # Misc
    parser.add_argument('--num-workers', type=int, default=DEFAULT_CONFIG['num_workers'],
                        help='Number of data loading workers')
    parser.add_argument('--seed', type=int, default=DEFAULT_CONFIG['seed'],
                        help='Random seed')
    
    args = parser.parse_args()
    
    # Build config from args
    config = DEFAULT_CONFIG.copy()
    config.update({
        'data_dir': args.data_dir,
        'val_split': args.val_split,
        'base_channels': args.base_channels,
        'batch_size': args.batch_size,
        'epochs': args.epochs,
        'learning_rate': args.lr,
        'weight_decay': args.weight_decay,
        'lambda_unit': args.lambda_unit,
        'lambda_recon': args.lambda_recon,
        'lambda_fa': args.lambda_fa,
        'weight_scheme': args.weight_scheme,
        'log_transform_intensity': args.log_transform_intensity,
        'augment': not args.no_augment,
        'output_dir': args.output_dir,
        'save_every': args.save_every,
        'resume': args.resume,
        'num_workers': args.num_workers,
        'seed': args.seed,
    })
    
    # Run training
    train(config)
