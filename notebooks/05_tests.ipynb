{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# NanoMAD ML - Test Suite\n",
    "\n",
    "This notebook stress-tests all components in Phase_6.0 to verify everything works correctly.\n",
    "\n",
    "**What it tests:**\n",
    "- Module imports\n",
    "- core_shell.py: Particle creation, diffraction computation, ground truth labels\n",
    "- mad_model.py: CNN architecture (forward pass with dummy data) - *requires PyTorch*\n",
    "- mad_loss.py: Loss function, F_N reconstruction - *requires PyTorch*\n",
    "- data_augmentation.py: Augmentation transforms\n",
    "- visualization.py: Plotting functions\n",
    "- ScatteringFactors: f'/f'' data loading\n",
    "- generate_training_data.py: Single particle generation\n",
    "\n",
    "**What it does NOT do:**\n",
    "- Train a full model (too slow)\n",
    "- Generate large datasets (too slow)\n",
    "- Run 3D inference on large volumes\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch not available - PyTorch-dependent tests will be skipped\n",
      "\n",
      "Test framework ready.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import traceback\n",
    "\n",
    "# Add parent directory to path for imports\n",
    "sys.path.insert(0, str(Path('.').resolve().parent))\n",
    "\n",
    "# Check if PyTorch is available\n",
    "try:\n",
    "    import torch\n",
    "    TORCH_AVAILABLE = True\n",
    "    print(f\"PyTorch {torch.__version__} available (CUDA: {torch.cuda.is_available()})\")\n",
    "except ImportError:\n",
    "    TORCH_AVAILABLE = False\n",
    "    print(\"PyTorch not available - PyTorch-dependent tests will be skipped\")\n",
    "\n",
    "# Test tracking\n",
    "test_results = []\n",
    "\n",
    "def run_test(name, func, requires_torch=False):\n",
    "    \"\"\"Run a test and track results.\"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"TEST: {name}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    if requires_torch and not TORCH_AVAILABLE:\n",
    "        print(f\"  SKIPPED: Requires PyTorch\")\n",
    "        test_results.append((name, 'SKIPPED', 'Requires PyTorch'))\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        func()\n",
    "        print(f\"\\n  PASSED: {name}\")\n",
    "        test_results.append((name, 'PASSED', None))\n",
    "    except Exception as e:\n",
    "        print(f\"\\n  FAILED: {name}\")\n",
    "        print(f\"  Error: {e}\")\n",
    "        traceback.print_exc()\n",
    "        test_results.append((name, 'FAILED', str(e)))\n",
    "\n",
    "print(\"\\nTest framework ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "---\n",
    "## Test 1: Core Module Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "TEST: Core Module Imports\n",
      "======================================================================\n",
      "  core_shell.py imports OK\n",
      "  visualization.py imports OK\n",
      "  augmentation.py imports OK\n",
      "  thomson_factors.py data loaded OK\n",
      "\n",
      "  PASSED: Core Module Imports\n"
     ]
    }
   ],
   "source": [
    "def test_core_imports():\n",
    "    \"\"\"Test that all core modules can be imported.\"\"\"\n",
    "    \n",
    "    # core_shell.py\n",
    "    from src.core_shell import (\n",
    "        create_particle_with_shape,\n",
    "        apply_displacement_to_particle,\n",
    "        create_layered_displacement_field,\n",
    "        create_random_strain_field,\n",
    "        compute_diffraction_oversampled_cropped,\n",
    "        compute_ground_truth_labels,\n",
    "        extract_label_patches,\n",
    "        ScatteringFactors,\n",
    "        get_total_density,\n",
    "        compute_f0_thomson,\n",
    "        SPECIES_NI,\n",
    "        SPECIES_FE,\n",
    "    )\n",
    "    print(\"  core_shell.py imports OK\")\n",
    "    \n",
    "    # visualization.py\n",
    "    from src.visualization import (\n",
    "        plot_particle,\n",
    "        plot_diffraction,\n",
    "        plot_diffraction_multi_energy,\n",
    "        plot_reconstruction,\n",
    "        plot_block_analysis,\n",
    "    )\n",
    "    print(\"  visualization.py imports OK\")\n",
    "    \n",
    "    # augmentation.py (renamed from data_augmentation.py)\n",
    "    from src.augmentation import (\n",
    "        augment_sample,\n",
    "        random_rot90,\n",
    "        random_flip,\n",
    "        random_intensity_scale,\n",
    "        add_poisson_noise,\n",
    "    )\n",
    "    print(\"  augmentation.py imports OK\")\n",
    "    \n",
    "    # thomson_factors.py (renamed from d_fthomson_IT92.py)\n",
    "    from data.thomson_factors import d_fthomson_IT92\n",
    "    assert 'Ni' in d_fthomson_IT92, \"Missing Ni in Thomson table\"\n",
    "    assert 'Fe' in d_fthomson_IT92, \"Missing Fe in Thomson table\"\n",
    "    print(\"  thomson_factors.py data loaded OK\")\n",
    "    \n",
    "run_test(\"Core Module Imports\", test_core_imports)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "---\n",
    "## Test 2: PyTorch Module Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "TEST: PyTorch Module Imports\n",
      "======================================================================\n",
      "  SKIPPED: Requires PyTorch\n"
     ]
    }
   ],
   "source": [
    "def test_torch_imports():\n",
    "    \"\"\"Test that PyTorch-dependent modules can be imported.\"\"\"\n",
    "    \n",
    "    import torch\n",
    "    print(f\"  PyTorch {torch.__version__}\")\n",
    "    print(f\"  CUDA available: {torch.cuda.is_available()}\")\n",
    "    \n",
    "    # mad_model.py\n",
    "    from src.mad_model import MADNet, count_parameters\n",
    "    print(\"  mad_model.py imports OK\")\n",
    "    \n",
    "    # mad_loss.py\n",
    "    from src.mad_loss import MADPhysicsLoss, compute_F_N\n",
    "    print(\"  mad_loss.py imports OK\")\n",
    "    \n",
    "    # train.py (renamed from train_cnn.py)\n",
    "    from src.train import MADDataset\n",
    "    print(\"  train.py imports OK\")\n",
    "    \n",
    "run_test(\"PyTorch Module Imports\", test_torch_imports, requires_torch=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "---\n",
    "## Test 3: Scattering Factors Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "TEST: Scattering Factors\n",
      "======================================================================\n",
      "Loading scattering factors for Ni from ../data/Nickel.f1f2...\n",
      "  Loaded 4601 data points\n",
      "  Energy range: 2000.0 - 25000.0 eV\n",
      "Loading scattering factors for Fe from ../data/Iron.f1f2...\n",
      "  Loaded 4601 data points\n",
      "  Energy range: 2000.0 - 25000.0 eV\n",
      "  ScatteringFactors loaded OK\n",
      "  Ni at 8333 eV: f'=-7.941, f''=2.521\n",
      "  Fe at 8333 eV: f'=-0.831, f''=3.025\n",
      "  f0(Q=0): Ni=28.0, Fe=26.0\n",
      "  All scattering factor values reasonable\n",
      "\n",
      "  PASSED: Scattering Factors\n"
     ]
    }
   ],
   "source": [
    "def test_scattering_factors():\n",
    "    \"\"\"Test scattering factor data loading and interpolation.\"\"\"\n",
    "    from src.core_shell import ScatteringFactors, compute_f0_thomson\n",
    "    \n",
    "    # Load scattering factors\n",
    "    sf = ScatteringFactors(data_dir='../data')\n",
    "    print(\"  ScatteringFactors loaded OK\")\n",
    "    \n",
    "    # Test f'/f'' at Ni K-edge\n",
    "    E = 8333  # Ni K-edge\n",
    "    fp_Ni = sf.get_f_prime('Ni', E)\n",
    "    fpp_Ni = sf.get_f_double_prime('Ni', E)\n",
    "    print(f\"  Ni at {E} eV: f'={fp_Ni:.3f}, f''={fpp_Ni:.3f}\")\n",
    "    \n",
    "    fp_Fe = sf.get_f_prime('Fe', E)\n",
    "    fpp_Fe = sf.get_f_double_prime('Fe', E)\n",
    "    print(f\"  Fe at {E} eV: f'={fp_Fe:.3f}, f''={fpp_Fe:.3f}\")\n",
    "    \n",
    "    # Test Thomson scattering (from core_shell)\n",
    "    # Signature: compute_f0_thomson(element, q_magnitude)\n",
    "    f0_Ni = compute_f0_thomson('Ni', 0.0)\n",
    "    f0_Fe = compute_f0_thomson('Fe', 0.0)\n",
    "    print(f\"  f0(Q=0): Ni={f0_Ni:.1f}, Fe={f0_Fe:.1f}\")\n",
    "    \n",
    "    # Verify values are reasonable\n",
    "    assert -10 < fp_Ni < 0, f\"Ni f' should be negative near edge, got {fp_Ni}\"\n",
    "    assert fpp_Ni > 0, f\"Ni f'' should be positive, got {fpp_Ni}\"\n",
    "    assert abs(f0_Ni - 28) < 1, f\"Ni f0 should be ~28, got {f0_Ni}\"\n",
    "    assert abs(f0_Fe - 26) < 1, f\"Fe f0 should be ~26, got {f0_Fe}\"\n",
    "    print(\"  All scattering factor values reasonable\")\n",
    "\n",
    "run_test(\"Scattering Factors\", test_scattering_factors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "---\n",
    "## Test 4: Particle Creation (All Types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "TEST: Particle Creation (All Types)\n",
      "======================================================================\n",
      "  circle: shape=(2, 64, 64), dtype=float64, Ni=665, Fe=44\n",
      "  hexagon: shape=(2, 64, 64), dtype=float64, Ni=143, Fe=9\n",
      "  polygon: shape=(2, 64, 64), dtype=float64, Ni=410, Fe=30\n",
      "  polygon_centrosymmetric: shape=(2, 64, 64), dtype=float64, Ni=198, Fe=30\n",
      "\n",
      "  PASSED: Particle Creation (All Types)\n"
     ]
    }
   ],
   "source": [
    "def test_particle_creation():\n",
    "    \"\"\"Test particle creation for all shape types.\"\"\"\n",
    "    from src.core_shell import create_particle_with_shape, get_total_density, SPECIES_NI, SPECIES_FE\n",
    "    \n",
    "    GRID_SIZE = 64  # Small for speed\n",
    "    shape_types = ['circle', 'hexagon', 'polygon', 'polygon_centrosymmetric']\n",
    "    \n",
    "    for shape_type in shape_types:\n",
    "        # Shape-specific params\n",
    "        if shape_type == 'hexagon':\n",
    "            params = {'anisotropy': 1.1}\n",
    "        elif shape_type in ['polygon', 'polygon_centrosymmetric']:\n",
    "            params = {'n_vertices': 6}\n",
    "        else:\n",
    "            params = {}\n",
    "        \n",
    "        particle, info = create_particle_with_shape(\n",
    "            grid_size=GRID_SIZE,\n",
    "            shape_type=shape_type,\n",
    "            outer_radius=15,\n",
    "            core_fraction=0.5,\n",
    "            pixel_size=5.0,\n",
    "            shape_params=params,\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        # Verify structure\n",
    "        assert particle.shape == (2, GRID_SIZE, GRID_SIZE), f\"Wrong shape: {particle.shape}\"\n",
    "        # Particle can be real (before displacement) or complex (after)\n",
    "        assert particle.dtype in [np.float64, np.complex128], f\"Wrong dtype: {particle.dtype}\"\n",
    "        \n",
    "        # Verify composition\n",
    "        ni_total = np.abs(particle[SPECIES_NI]).sum()\n",
    "        fe_total = np.abs(particle[SPECIES_FE]).sum()\n",
    "        assert ni_total > 0, \"No Ni content\"\n",
    "        assert fe_total > 0, \"No Fe content\"\n",
    "        \n",
    "        # Verify info dict\n",
    "        assert 'outer_mask' in info\n",
    "        assert 'core_mask' in info\n",
    "        assert 'shell_mask' in info\n",
    "        \n",
    "        print(f\"  {shape_type}: shape={particle.shape}, dtype={particle.dtype}, Ni={ni_total:.0f}, Fe={fe_total:.0f}\")\n",
    "\n",
    "run_test(\"Particle Creation (All Types)\", test_particle_creation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "---\n",
    "## Test 5: Strain Field Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "TEST: Strain Field Generation\n",
      "======================================================================\n",
      "  Particle created\n",
      "  Analytic displacement: range [0.00, 0.62] A\n",
      "  Random strain: range [-0.19, 0.20] A\n",
      "  Displacement applied to particle\n",
      "\n",
      "  PASSED: Strain Field Generation\n"
     ]
    }
   ],
   "source": [
    "def test_strain_fields():\n",
    "    \"\"\"Test displacement/strain field generation.\"\"\"\n",
    "    from src.core_shell import (\n",
    "        create_particle_with_shape,\n",
    "        create_layered_displacement_field,\n",
    "        create_random_strain_field,\n",
    "        apply_displacement_to_particle,\n",
    "    )\n",
    "    \n",
    "    GRID_SIZE = 64\n",
    "    PIXEL_SIZE = 5.0\n",
    "    Q_BRAGG = 3.09\n",
    "    \n",
    "    # Create particle\n",
    "    particle, info = create_particle_with_shape(\n",
    "        grid_size=GRID_SIZE,\n",
    "        shape_type='circle',\n",
    "        outer_radius=15,\n",
    "        core_fraction=0.5,\n",
    "        pixel_size=PIXEL_SIZE,\n",
    "        verbose=False\n",
    "    )\n",
    "    print(\"  Particle created\")\n",
    "    \n",
    "    # Create analytic displacement\n",
    "    displacement_analytic, disp_info = create_layered_displacement_field(\n",
    "        core_mask=info['core_mask'],\n",
    "        outer_mask=info['outer_mask'],\n",
    "        pixel_size=PIXEL_SIZE,\n",
    "        interface_amplitude=1.0,\n",
    "        surface_amplitude=0.5,\n",
    "        verbose=False\n",
    "    )\n",
    "    assert displacement_analytic.shape == (GRID_SIZE, GRID_SIZE)\n",
    "    print(f\"  Analytic displacement: range [{displacement_analytic.min():.2f}, {displacement_analytic.max():.2f}] A\")\n",
    "    \n",
    "    # Create random strain\n",
    "    displacement_random = create_random_strain_field(\n",
    "        grid_size=GRID_SIZE,\n",
    "        pixel_size=PIXEL_SIZE,\n",
    "        displacement_amplitude=0.2,\n",
    "        correlation_length=0.1,\n",
    "        verbose=False\n",
    "    )\n",
    "    assert displacement_random.shape == (GRID_SIZE, GRID_SIZE)\n",
    "    print(f\"  Random strain: range [{displacement_random.min():.2f}, {displacement_random.max():.2f}] A\")\n",
    "    \n",
    "    # Apply displacement\n",
    "    total_displacement = displacement_analytic + displacement_random\n",
    "    particle_strained = apply_displacement_to_particle(\n",
    "        particle=particle,\n",
    "        displacement=total_displacement,\n",
    "        q_bragg_magnitude=Q_BRAGG\n",
    "    )\n",
    "    assert particle_strained.shape == particle.shape\n",
    "    assert particle_strained.dtype == np.complex128\n",
    "    print(\"  Displacement applied to particle\")\n",
    "\n",
    "run_test(\"Strain Field Generation\", test_strain_fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "---\n",
    "## Test 6: Diffraction Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell-13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "TEST: Diffraction Computation\n",
      "======================================================================\n",
      "Loading scattering factors for Ni from ../data/Nickel.f1f2...\n",
      "  Loaded 4601 data points\n",
      "  Energy range: 2000.0 - 25000.0 eV\n",
      "Loading scattering factors for Fe from ../data/Iron.f1f2...\n",
      "  Loaded 4601 data points\n",
      "  Energy range: 2000.0 - 25000.0 eV\n",
      "  Strained particle created\n",
      "  E=8313 eV: shape=(32, 32), I_max=2.31e+08\n",
      "  E=8333 eV: shape=(32, 32), I_max=1.92e+08\n",
      "  E=8348 eV: shape=(32, 32), I_max=2.31e+08\n",
      "\n",
      "  PASSED: Diffraction Computation\n"
     ]
    }
   ],
   "source": [
    "def test_diffraction():\n",
    "    \"\"\"Test multi-energy diffraction computation.\"\"\"\n",
    "    from src.core_shell import (\n",
    "        create_particle_with_shape,\n",
    "        apply_displacement_to_particle,\n",
    "        create_layered_displacement_field,\n",
    "        compute_diffraction_oversampled_cropped,\n",
    "        ScatteringFactors,\n",
    "    )\n",
    "    \n",
    "    GRID_SIZE = 64\n",
    "    OUTPUT_SIZE = 32\n",
    "    PIXEL_SIZE = 5.0\n",
    "    Q_BRAGG = 3.09\n",
    "    ENERGIES = [8313, 8333, 8348]  # 3 energies for speed\n",
    "    \n",
    "    sf = ScatteringFactors(data_dir='../data')\n",
    "    \n",
    "    # Create strained particle\n",
    "    particle, info = create_particle_with_shape(\n",
    "        grid_size=GRID_SIZE, shape_type='circle',\n",
    "        outer_radius=15, core_fraction=0.5,\n",
    "        pixel_size=PIXEL_SIZE, verbose=False\n",
    "    )\n",
    "    displacement, _ = create_layered_displacement_field(\n",
    "        core_mask=info['core_mask'], outer_mask=info['outer_mask'],\n",
    "        pixel_size=PIXEL_SIZE, verbose=False\n",
    "    )\n",
    "    particle_strained = apply_displacement_to_particle(\n",
    "        particle, displacement, Q_BRAGG\n",
    "    )\n",
    "    print(\"  Strained particle created\")\n",
    "    \n",
    "    # Compute diffraction\n",
    "    diffractions = compute_diffraction_oversampled_cropped(\n",
    "        particle=particle_strained,\n",
    "        energies=ENERGIES,\n",
    "        pixel_size=PIXEL_SIZE,\n",
    "        scattering_factors=sf,\n",
    "        output_size=OUTPUT_SIZE,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    assert len(diffractions) == len(ENERGIES), f\"Wrong number of patterns: {len(diffractions)}\"\n",
    "    for E, D in diffractions.items():\n",
    "        assert D.shape == (OUTPUT_SIZE, OUTPUT_SIZE), f\"Wrong shape at E={E}: {D.shape}\"\n",
    "        assert np.iscomplexobj(D), f\"Diffraction should be complex at E={E}\"\n",
    "        intensity = np.abs(D)**2\n",
    "        assert intensity.max() > 0, f\"Zero intensity at E={E}\"\n",
    "        print(f\"  E={E} eV: shape={D.shape}, I_max={intensity.max():.2e}\")\n",
    "\n",
    "run_test(\"Diffraction Computation\", test_diffraction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "---\n",
    "## Test 7: Ground Truth Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cell-15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "TEST: Ground Truth Labels\n",
      "======================================================================\n",
      "  All expected label keys present\n",
      "  Full keys: ['F_T_mag', 'F_A_mag', 'delta_phi', 'cos_delta_phi', 'sin_delta_phi', 'F_N_mag', 'F_N_mag_derived', 'f0_Ni', 'f0_Fe']\n",
      "  sin^2(dphi) + cos^2(dphi) = 1 verified\n",
      "  F_N = sqrt(F_T^2 + F_A^2 - 2*F_T*F_A*cos(dphi)) verified\n",
      "  Extracted patches shape: (2, 2, 16, 16, 4)\n",
      "\n",
      "  PASSED: Ground Truth Labels\n"
     ]
    }
   ],
   "source": [
    "def test_ground_truth():\n",
    "    \"\"\"Test ground truth label computation.\"\"\"\n",
    "    from src.core_shell import (\n",
    "        create_particle_with_shape,\n",
    "        apply_displacement_to_particle,\n",
    "        create_layered_displacement_field,\n",
    "        compute_ground_truth_labels,\n",
    "        extract_label_patches,\n",
    "    )\n",
    "    \n",
    "    GRID_SIZE = 64\n",
    "    OUTPUT_SIZE = 32\n",
    "    PATCH_SIZE = 16\n",
    "    PIXEL_SIZE = 5.0\n",
    "    Q_BRAGG = 3.09\n",
    "    \n",
    "    # Create particle\n",
    "    particle, info = create_particle_with_shape(\n",
    "        grid_size=GRID_SIZE, shape_type='circle',\n",
    "        outer_radius=15, core_fraction=0.5,\n",
    "        pixel_size=PIXEL_SIZE, verbose=False\n",
    "    )\n",
    "    displacement, _ = create_layered_displacement_field(\n",
    "        core_mask=info['core_mask'], outer_mask=info['outer_mask'],\n",
    "        pixel_size=PIXEL_SIZE, verbose=False\n",
    "    )\n",
    "    particle_strained = apply_displacement_to_particle(\n",
    "        particle, displacement, Q_BRAGG\n",
    "    )\n",
    "    \n",
    "    # Compute ground truth\n",
    "    labels = compute_ground_truth_labels(\n",
    "        particle=particle_strained,\n",
    "        pixel_size=PIXEL_SIZE,\n",
    "        output_size=OUTPUT_SIZE,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    # Check all expected keys\n",
    "    expected_keys = ['F_T_mag', 'F_A_mag', 'F_N_mag', 'delta_phi', 'sin_delta_phi', 'cos_delta_phi']\n",
    "    for key in expected_keys:\n",
    "        assert key in labels, f\"Missing key: {key}\"\n",
    "        assert labels[key].shape == (OUTPUT_SIZE, OUTPUT_SIZE), f\"Wrong shape for {key}\"\n",
    "    print(f\"  All expected label keys present\")\n",
    "    print(f\"  Full keys: {list(labels.keys())}\")\n",
    "    \n",
    "    # Verify sin^2+cos^2 = 1\n",
    "    sin2_cos2 = labels['sin_delta_phi']**2 + labels['cos_delta_phi']**2\n",
    "    assert np.allclose(sin2_cos2, 1.0, atol=1e-6), \"sin^2+cos^2 != 1\"\n",
    "    print(\"  sin^2(dphi) + cos^2(dphi) = 1 verified\")\n",
    "    \n",
    "    # Verify F_N derivation\n",
    "    F_T = labels['F_T_mag']\n",
    "    F_A = labels['F_A_mag']\n",
    "    F_N = labels['F_N_mag']\n",
    "    delta_phi = labels['delta_phi']\n",
    "    \n",
    "    F_N_check = np.sqrt(np.maximum(F_T**2 + F_A**2 - 2*F_T*F_A*np.cos(delta_phi), 0))\n",
    "    assert np.allclose(F_N, F_N_check, rtol=1e-4), \"F_N derivation mismatch\"\n",
    "    print(\"  F_N = sqrt(F_T^2 + F_A^2 - 2*F_T*F_A*cos(dphi)) verified\")\n",
    "    \n",
    "    # Extract patches\n",
    "    patches = extract_label_patches(\n",
    "        labels=labels,\n",
    "        patch_size=PATCH_SIZE,\n",
    "        label_keys=['F_T_mag', 'F_A_mag', 'sin_delta_phi', 'cos_delta_phi']\n",
    "    )\n",
    "    n_patches_per_dim = OUTPUT_SIZE // PATCH_SIZE\n",
    "    n_patches = n_patches_per_dim ** 2\n",
    "    expected_shape = (n_patches_per_dim, n_patches_per_dim, PATCH_SIZE, PATCH_SIZE, 4)\n",
    "    assert patches.shape == expected_shape, f\"Patches shape {patches.shape} != expected {expected_shape}\"\n",
    "    print(f\"  Extracted patches shape: {patches.shape}\")\n",
    "\n",
    "run_test(\"Ground Truth Labels\", test_ground_truth)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "---\n",
    "## Test 8: CNN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cell-17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "TEST: CNN Architecture\n",
      "======================================================================\n",
      "  SKIPPED: Requires PyTorch\n"
     ]
    }
   ],
   "source": [
    "def test_cnn_architecture():\n",
    "    \"\"\"Test CNN model forward pass.\"\"\"\n",
    "    import torch\n",
    "    from src.mad_model import MADNet, count_parameters\n",
    "    \n",
    "    # Create model\n",
    "    model = MADNet(\n",
    "        in_channels=8,\n",
    "        out_channels=4,\n",
    "        base_filters=32\n",
    "    )\n",
    "    model.eval()\n",
    "    \n",
    "    n_params = count_parameters(model)\n",
    "    print(f\"  MADNet created: {n_params:,} parameters\")\n",
    "    \n",
    "    # Test forward pass\n",
    "    batch_size = 2\n",
    "    patch_size = 16\n",
    "    n_energies = 8\n",
    "    \n",
    "    # Inputs\n",
    "    x = torch.randn(batch_size, n_energies, patch_size, patch_size)\n",
    "    f_prime = torch.randn(batch_size, n_energies)\n",
    "    f_double_prime = torch.randn(batch_size, n_energies)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        y = model(x, f_prime, f_double_prime)\n",
    "    \n",
    "    expected_shape = (batch_size, 4, patch_size, patch_size)\n",
    "    assert y.shape == expected_shape, f\"Wrong output shape: {y.shape} vs {expected_shape}\"\n",
    "    print(f\"  Forward pass: input {x.shape} -> output {y.shape}\")\n",
    "    \n",
    "    # Check output ranges\n",
    "    # Channels 0,1 (magnitudes) should be >= 0 after softplus\n",
    "    # Channels 2,3 (sin/cos) should be in [-1, 1] after tanh\n",
    "    assert (y[:, :2] >= 0).all(), \"Magnitudes should be non-negative\"\n",
    "    assert (y[:, 2:].abs() <= 1).all(), \"sin/cos should be in [-1, 1]\"\n",
    "    print(\"  Output ranges correct (magnitudes >= 0, sin/cos in [-1, 1])\")\n",
    "\n",
    "run_test(\"CNN Architecture\", test_cnn_architecture, requires_torch=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "---\n",
    "## Test 9: Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cell-19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "TEST: Loss Function\n",
      "======================================================================\n",
      "  SKIPPED: Requires PyTorch\n"
     ]
    }
   ],
   "source": [
    "def test_loss_function():\n",
    "    \"\"\"Test physics-informed loss function.\"\"\"\n",
    "    import torch\n",
    "    from src.mad_loss import MADPhysicsLoss, compute_F_N\n",
    "    \n",
    "    # Create loss\n",
    "    loss_fn = MADPhysicsLoss(\n",
    "        mag_weight=1.0,\n",
    "        phase_weight=1.0,\n",
    "        physics_weight=0.1,\n",
    "        normalize_weight=0.01\n",
    "    )\n",
    "    print(\"  MADPhysicsLoss created\")\n",
    "    \n",
    "    # Create dummy predictions and targets\n",
    "    batch_size = 4\n",
    "    patch_size = 16\n",
    "    \n",
    "    pred = torch.rand(batch_size, 4, patch_size, patch_size)\n",
    "    pred[:, :2] = pred[:, :2] * 5  # Magnitudes in [0, 5]\n",
    "    pred[:, 2:] = pred[:, 2:] * 2 - 1  # sin/cos in [-1, 1]\n",
    "    \n",
    "    target = torch.rand(batch_size, 4, patch_size, patch_size)\n",
    "    target[:, :2] = target[:, :2] * 5\n",
    "    target[:, 2:] = target[:, 2:] * 2 - 1\n",
    "    \n",
    "    f_prime = torch.randn(batch_size, 8)\n",
    "    f_double_prime = torch.abs(torch.randn(batch_size, 8))\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = loss_fn(pred, target, f_prime, f_double_prime)\n",
    "    \n",
    "    assert loss.ndim == 0, f\"Loss should be scalar, got shape {loss.shape}\"\n",
    "    assert loss.item() > 0, \"Loss should be positive\"\n",
    "    assert not torch.isnan(loss), \"Loss is NaN\"\n",
    "    print(f\"  Loss computation: {loss.item():.4f}\")\n",
    "    \n",
    "    # Test F_N computation\n",
    "    F_T = torch.rand(batch_size, patch_size, patch_size) * 100\n",
    "    F_A = torch.rand(batch_size, patch_size, patch_size) * 50\n",
    "    delta_phi = torch.rand(batch_size, patch_size, patch_size) * 2 * np.pi - np.pi\n",
    "    \n",
    "    F_N = compute_F_N(F_T, F_A, delta_phi)\n",
    "    \n",
    "    assert F_N.shape == F_T.shape, f\"F_N shape mismatch: {F_N.shape}\"\n",
    "    assert (F_N >= 0).all(), \"F_N should be non-negative\"\n",
    "    print(f\"  compute_F_N: range [{F_N.min():.2f}, {F_N.max():.2f}]\")\n",
    "\n",
    "run_test(\"Loss Function\", test_loss_function, requires_torch=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "---\n",
    "## Test 10: Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cell-21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "TEST: Data Augmentation\n",
      "======================================================================\n",
      "  random_rot90 OK\n",
      "  random_flip OK\n",
      "  random_intensity_scale OK\n",
      "  add_poisson_noise OK\n",
      "  augment_sample pipeline OK\n",
      "\n",
      "  PASSED: Data Augmentation\n"
     ]
    }
   ],
   "source": [
    "def test_augmentation():\n",
    "    \"\"\"Test physics-preserving augmentation.\"\"\"\n",
    "    from src.augmentation import (\n",
    "        augment_sample,\n",
    "        random_rot90,\n",
    "        random_flip,\n",
    "        random_intensity_scale,\n",
    "        add_poisson_noise,\n",
    "    )\n",
    "    \n",
    "    # Create dummy sample\n",
    "    patch_size = 16\n",
    "    n_energies = 8\n",
    "    \n",
    "    X = np.random.rand(patch_size, patch_size, n_energies).astype(np.float32) * 1000\n",
    "    Y = np.random.rand(patch_size, patch_size, 4).astype(np.float32)\n",
    "    Y[..., :2] *= 10  # Magnitudes\n",
    "    Y[..., 2:] = Y[..., 2:] * 2 - 1  # sin/cos\n",
    "    \n",
    "    # Test individual augmentations\n",
    "    X_rot, Y_rot = random_rot90(X.copy(), Y.copy())\n",
    "    assert X_rot.shape == X.shape, f\"rot90 changed X shape\"\n",
    "    print(\"  random_rot90 OK\")\n",
    "    \n",
    "    X_flip, Y_flip = random_flip(X.copy(), Y.copy())\n",
    "    assert X_flip.shape == X.shape, f\"flip changed X shape\"\n",
    "    print(\"  random_flip OK\")\n",
    "    \n",
    "    X_scaled, Y_scaled = random_intensity_scale(X.copy(), Y.copy())\n",
    "    assert X_scaled.shape == X.shape, f\"intensity_scale changed X shape\"\n",
    "    print(\"  random_intensity_scale OK\")\n",
    "    \n",
    "    # add_poisson_noise signature: (intensity, noise_level=None, noise_range=(0.01, 0.1))\n",
    "    X_noisy = add_poisson_noise(X.copy(), noise_level=0.05)\n",
    "    assert X_noisy.shape == X.shape, f\"poisson_noise changed X shape\"\n",
    "    assert (X_noisy >= 0).all(), \"Poisson noise made negative values\"\n",
    "    print(\"  add_poisson_noise OK\")\n",
    "    \n",
    "    # Test full augmentation pipeline\n",
    "    X_aug, Y_aug = augment_sample(X.copy(), Y.copy())\n",
    "    assert X_aug.shape == X.shape, f\"augment_sample changed X shape\"\n",
    "    assert Y_aug.shape == Y.shape, f\"augment_sample changed Y shape\"\n",
    "    print(\"  augment_sample pipeline OK\")\n",
    "\n",
    "run_test(\"Data Augmentation\", test_augmentation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "---\n",
    "## Test 11: MAD Equation Consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cell-23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "TEST: MAD Equation Consistency\n",
      "======================================================================\n",
      "Loading scattering factors for Ni from ../data/Nickel.f1f2...\n",
      "  Loaded 4601 data points\n",
      "  Energy range: 2000.0 - 25000.0 eV\n",
      "Loading scattering factors for Fe from ../data/Iron.f1f2...\n",
      "  Loaded 4601 data points\n",
      "  Energy range: 2000.0 - 25000.0 eV\n",
      "  Testing MAD equation at each energy...\n",
      "    E=8313 eV: correlation = 1.0000\n",
      "    E=8333 eV: correlation = 1.0000\n",
      "    E=8348 eV: correlation = 1.0000\n",
      "  MAD equation produces correlated intensities\n",
      "\n",
      "  PASSED: MAD Equation Consistency\n"
     ]
    }
   ],
   "source": [
    "def test_mad_equation():\n",
    "    \"\"\"Verify MAD equation: I = |F_T|^2 + (f'^2+f''^2)|F_A|^2/f0^2 + 2|F_T||F_A|/f0*[f'cos(dphi)+f''sin(dphi)]\"\"\"\n",
    "    from src.core_shell import (\n",
    "        create_particle_with_shape,\n",
    "        apply_displacement_to_particle,\n",
    "        create_layered_displacement_field,\n",
    "        compute_diffraction_oversampled_cropped,\n",
    "        compute_ground_truth_labels,\n",
    "        ScatteringFactors,\n",
    "        compute_f0_thomson,\n",
    "    )\n",
    "    \n",
    "    GRID_SIZE = 64\n",
    "    OUTPUT_SIZE = 32\n",
    "    PIXEL_SIZE = 5.0\n",
    "    Q_BRAGG = 3.09\n",
    "    ENERGIES = [8313, 8333, 8348]\n",
    "    \n",
    "    sf = ScatteringFactors()  # Auto-detect data directory\n",
    "    \n",
    "    # Create particle\n",
    "    particle, info = create_particle_with_shape(\n",
    "        grid_size=GRID_SIZE, shape_type='circle',\n",
    "        outer_radius=15, core_fraction=0.5,\n",
    "        pixel_size=PIXEL_SIZE, verbose=False\n",
    "    )\n",
    "    displacement, _ = create_layered_displacement_field(\n",
    "        core_mask=info['core_mask'], outer_mask=info['outer_mask'],\n",
    "        pixel_size=PIXEL_SIZE, verbose=False\n",
    "    )\n",
    "    particle_strained = apply_displacement_to_particle(\n",
    "        particle, displacement, Q_BRAGG\n",
    "    )\n",
    "    \n",
    "    # Compute diffraction\n",
    "    diffractions = compute_diffraction_oversampled_cropped(\n",
    "        particle=particle_strained,\n",
    "        energies=ENERGIES,\n",
    "        pixel_size=PIXEL_SIZE,\n",
    "        scattering_factors=sf,\n",
    "        output_size=OUTPUT_SIZE,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    # Compute ground truth\n",
    "    labels = compute_ground_truth_labels(\n",
    "        particle=particle_strained,\n",
    "        pixel_size=PIXEL_SIZE,\n",
    "        output_size=OUTPUT_SIZE,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    F_T = labels['F_T_mag']\n",
    "    F_A = labels['F_A_mag']\n",
    "    delta_phi = labels['delta_phi']\n",
    "    \n",
    "    # Test MAD equation at each energy\n",
    "    print(\"  Testing MAD equation at each energy...\")\n",
    "    # Signature: compute_f0_thomson(element, q_magnitude)\n",
    "    f0 = compute_f0_thomson('Ni', 0.0)  # Approximate f0\n",
    "    \n",
    "    for E in ENERGIES:\n",
    "        fp = sf.get_f_prime('Ni', E)\n",
    "        fpp = sf.get_f_double_prime('Ni', E)\n",
    "        \n",
    "        # Predicted intensity from MAD equation\n",
    "        I_mad = (\n",
    "            F_T**2 +\n",
    "            (fp**2 + fpp**2) * (F_A / f0)**2 +\n",
    "            2 * F_T * F_A / f0 * (fp * np.cos(delta_phi) + fpp * np.sin(delta_phi))\n",
    "        )\n",
    "        \n",
    "        # Actual intensity from diffraction\n",
    "        I_actual = np.abs(diffractions[E])**2\n",
    "        \n",
    "        # They should be correlated (not exactly equal due to approximations)\n",
    "        corr = np.corrcoef(I_mad.flatten(), I_actual.flatten())[0, 1]\n",
    "        print(f\"    E={E} eV: correlation = {corr:.4f}\")\n",
    "        \n",
    "        # Correlation should be high (> 0.5 typically, allowing for Q-dependent f0 approximation)\n",
    "        assert corr > 0.5, f\"MAD equation correlation too low at E={E}: {corr}\"\n",
    "    \n",
    "    print(\"  MAD equation produces correlated intensities\")\n",
    "\n",
    "run_test(\"MAD Equation Consistency\", test_mad_equation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-24",
   "metadata": {},
   "source": [
    "---\n",
    "## Test 12: Training Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cell-25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "TEST: Training Data Generation\n",
      "======================================================================\n",
      "Loading scattering factors for Ni from ../data/Nickel.f1f2...\n",
      "  Loaded 4601 data points\n",
      "  Energy range: 2000.0 - 25000.0 eV\n",
      "Loading scattering factors for Fe from ../data/Iron.f1f2...\n",
      "  Loaded 4601 data points\n",
      "  Energy range: 2000.0 - 25000.0 eV\n",
      "  Sampled shape_type: polygon_centrosymmetric\n",
      "  Params keys: ['shape_type', 'composition_mode', 'outer_radius', 'core_fraction', 'interface_amplitude', 'surface_amplitude', 'random_amplitude', 'random_correlation', 'n_vertices', 'truncation_fraction', 'truncation_angle', 'core_offset', 'composition_params', 'module_noise_amplitude']\n",
      "  process_particle returned data with keys: ['X', 'Y', 'f_prime', 'f_double_prime', 'energies', 'shape_type', 'params', 'n_patches']\n",
      "  Generated particle_0000.npz\n",
      "  All required keys present: ['X', 'Y', 'f_prime', 'f_double_prime', 'energies', 'shape_type', 'params_json']\n",
      "  X shape: (64, 16, 16, 8), Y shape: (64, 16, 16, 4)\n",
      "  Data types correct (float32)\n",
      "\n",
      "  PASSED: Training Data Generation\n"
     ]
    }
   ],
   "source": [
    "def test_training_data_generation():\n",
    "    \"\"\"Test single particle training data generation using process_particle.\"\"\"\n",
    "    import tempfile\n",
    "    from pathlib import Path\n",
    "    \n",
    "    # Import generation functions\n",
    "    from src.generate_data import process_particle, save_particle_data, sample_parameters, sample_shape_type\n",
    "    from src.core_shell import ScatteringFactors\n",
    "    \n",
    "    sf = ScatteringFactors()  # Auto-detect data directory\n",
    "    \n",
    "    with tempfile.TemporaryDirectory() as tmpdir:\n",
    "        output_dir = Path(tmpdir)\n",
    "        \n",
    "        # Sample random parameters for a particle\n",
    "        shape_type = sample_shape_type()\n",
    "        params = sample_parameters(shape_type)\n",
    "        print(f\"  Sampled shape_type: {shape_type}\")\n",
    "        print(f\"  Params keys: {list(params.keys())}\")\n",
    "        \n",
    "        # Generate one particle\n",
    "        # Signature: process_particle(params, scattering_factors, verbose=False)\n",
    "        data = process_particle(\n",
    "            params=params,\n",
    "            scattering_factors=sf,\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        assert data is not None, \"process_particle returned None\"\n",
    "        print(f\"  process_particle returned data with keys: {list(data.keys())}\")\n",
    "        \n",
    "        # Save and verify\n",
    "        save_particle_data(data, output_dir, particle_idx=0)\n",
    "        \n",
    "        # Check output file exists\n",
    "        output_file = output_dir / 'particle_0000.npz'\n",
    "        assert output_file.exists(), f\"Output file not created: {output_file}\"\n",
    "        print(f\"  Generated {output_file.name}\")\n",
    "        \n",
    "        # Load and verify\n",
    "        loaded = np.load(output_file)\n",
    "        \n",
    "        required_keys = ['X', 'Y', 'f_prime', 'f_double_prime', 'energies']\n",
    "        for key in required_keys:\n",
    "            assert key in loaded, f\"Missing key: {key}\"\n",
    "        print(f\"  All required keys present: {list(loaded.keys())}\")\n",
    "        \n",
    "        # Check shapes\n",
    "        X = loaded['X']\n",
    "        Y = loaded['Y']\n",
    "        print(f\"  X shape: {X.shape}, Y shape: {Y.shape}\")\n",
    "        \n",
    "        # Verify data types\n",
    "        assert X.dtype == np.float32, f\"X dtype wrong: {X.dtype}\"\n",
    "        assert Y.dtype == np.float32, f\"Y dtype wrong: {Y.dtype}\"\n",
    "        print(\"  Data types correct (float32)\")\n",
    "\n",
    "run_test(\"Training Data Generation\", test_training_data_generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-26",
   "metadata": {},
   "source": [
    "---\n",
    "## Test 13: Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cell-27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "TEST: Dataset Loading\n",
      "======================================================================\n",
      "  SKIPPED: Requires PyTorch\n"
     ]
    }
   ],
   "source": [
    "def test_dataset_loading():\n",
    "    \"\"\"Test MADDataset loading and batching.\"\"\"\n",
    "    import torch\n",
    "    from torch.utils.data import DataLoader\n",
    "    import tempfile\n",
    "    from pathlib import Path\n",
    "    from src.train import MADDataset\n",
    "    from src.generate_data import process_particle, save_particle_data, sample_parameters, sample_shape_type\n",
    "    from src.core_shell import ScatteringFactors\n",
    "    \n",
    "    sf = ScatteringFactors()  # Auto-detect data directory\n",
    "    \n",
    "    with tempfile.TemporaryDirectory() as tmpdir:\n",
    "        output_dir = Path(tmpdir)\n",
    "        \n",
    "        # Generate a few particles\n",
    "        for i in range(3):\n",
    "            shape_type = sample_shape_type()\n",
    "            params = sample_parameters(shape_type)\n",
    "            data = process_particle(\n",
    "                params=params,\n",
    "                scattering_factors=sf,\n",
    "                verbose=False\n",
    "            )\n",
    "            save_particle_data(data, output_dir, particle_idx=i)\n",
    "        print(f\"  Generated 3 particle files\")\n",
    "        \n",
    "        # Create dataset\n",
    "        dataset = MADDataset(output_dir)\n",
    "        print(f\"  MADDataset created with {len(dataset)} samples\")\n",
    "        \n",
    "        # Test single sample\n",
    "        X, Y, fp, fpp = dataset[0]\n",
    "        print(f\"  Single sample: X={X.shape}, Y={Y.shape}, fp={fp.shape}, fpp={fpp.shape}\")\n",
    "        \n",
    "        # Test DataLoader\n",
    "        loader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "        batch = next(iter(loader))\n",
    "        X_batch, Y_batch, fp_batch, fpp_batch = batch\n",
    "        print(f\"  Batch loading: X={X_batch.shape}, Y={Y_batch.shape}\")\n",
    "\n",
    "run_test(\"Dataset Loading\", test_dataset_loading, requires_torch=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ijlkuqc9y29",
   "metadata": {},
   "source": [
    "---\n",
    "## Test 14: Ellipse Shape Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "lpjn17l83gs",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "TEST: Ellipse Shape Creation\n",
      "======================================================================\n",
      "  Ellipse mask: 140 pixels, a=0.3, b=0.15 (normalized)\n",
      "  Circular aspect: 1.00, Elongated aspect: 0.36\n",
      "  Ellipse particle created: 412 pixels in outer region\n",
      "\n",
      "  PASSED: Ellipse Shape Creation\n"
     ]
    }
   ],
   "source": [
    "def test_ellipse_shape():\n",
    "    \"\"\"Test ellipse shape creation with various aspect ratios and rotations.\"\"\"\n",
    "    from src.core_shell import create_ellipse_mask, create_particle_with_shape\n",
    "    \n",
    "    GRID_SIZE = 64\n",
    "    \n",
    "    # Note: create_ellipse_mask expects NORMALIZED coordinates (0-1), not pixel coordinates!\n",
    "    # center: (0.5, 0.5) = center of grid\n",
    "    # semi_major/semi_minor: fraction of half-grid (0.3 = 30% of half-grid width)\n",
    "    \n",
    "    # Test 1: Basic ellipse mask creation with normalized coordinates\n",
    "    center_normalized = (0.5, 0.5)  # Center of grid in normalized coords\n",
    "    semi_major = 0.3   # 30% of half-grid = ~10 pixels for 64x64\n",
    "    semi_minor = 0.15  # 15% of half-grid = ~5 pixels for 64x64\n",
    "    rotation = 0.0\n",
    "    \n",
    "    mask, vertices = create_ellipse_mask(\n",
    "        grid_size=GRID_SIZE,\n",
    "        center=center_normalized,\n",
    "        semi_major=semi_major,\n",
    "        semi_minor=semi_minor,\n",
    "        rotation_angle=rotation,\n",
    "        seed=42,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    assert mask.shape == (GRID_SIZE, GRID_SIZE), f\"Wrong mask shape: {mask.shape}\"\n",
    "    assert mask.dtype == bool, f\"Wrong mask dtype: {mask.dtype}\"\n",
    "    assert mask.sum() > 0, \"Empty mask\"\n",
    "    print(f\"  Ellipse mask: {mask.sum()} pixels, a={semi_major}, b={semi_minor} (normalized)\")\n",
    "    \n",
    "    # Test 2: Verify aspect ratio affects shape\n",
    "    mask_circular, _ = create_ellipse_mask(GRID_SIZE, (0.5, 0.5), 0.25, 0.25, 0.0, 42, False)\n",
    "    mask_elongated, _ = create_ellipse_mask(GRID_SIZE, (0.5, 0.5), 0.35, 0.12, 0.0, 42, False)\n",
    "    \n",
    "    # Elongated should have different extent along axes\n",
    "    circular_extent_y = np.where(mask_circular.any(axis=1))[0]\n",
    "    circular_extent_x = np.where(mask_circular.any(axis=0))[0]\n",
    "    elongated_extent_y = np.where(mask_elongated.any(axis=1))[0]\n",
    "    elongated_extent_x = np.where(mask_elongated.any(axis=0))[0]\n",
    "    \n",
    "    circular_ratio = len(circular_extent_y) / len(circular_extent_x)\n",
    "    elongated_ratio = len(elongated_extent_y) / len(elongated_extent_x)\n",
    "    \n",
    "    print(f\"  Circular aspect: {circular_ratio:.2f}, Elongated aspect: {elongated_ratio:.2f}\")\n",
    "    assert abs(circular_ratio - 1.0) < 0.1, \"Circular should have ~1:1 aspect\"\n",
    "    assert elongated_ratio < 0.6, f\"Elongated should be flattened, got {elongated_ratio}\"\n",
    "    \n",
    "    # Test 3: Full particle creation with ellipse shape\n",
    "    # Note: create_particle_with_shape handles the coordinate conversion internally\n",
    "    particle, info = create_particle_with_shape(\n",
    "        grid_size=GRID_SIZE,\n",
    "        shape_type='ellipse',\n",
    "        outer_radius=18,\n",
    "        core_fraction=0.5,\n",
    "        pixel_size=5.0,\n",
    "        shape_params={'aspect_ratio': 0.6, 'rotation': np.pi/4},\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    assert particle.shape == (2, GRID_SIZE, GRID_SIZE), f\"Wrong particle shape: {particle.shape}\"\n",
    "    assert 'outer_mask' in info, \"Missing outer_mask in info\"\n",
    "    print(f\"  Ellipse particle created: {info['outer_mask'].sum()} pixels in outer region\")\n",
    "\n",
    "run_test(\"Ellipse Shape Creation\", test_ellipse_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vuajg83e6vg",
   "metadata": {},
   "source": [
    "---\n",
    "## Test 15: Boundary Validation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cxx9jazl1n",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "TEST: Boundary Validation Functions\n",
      "======================================================================\n",
      "  Centered particle (r=20) at (32, 32): fits=True\n",
      "  Large particle (r=35) at (32, 32): fits=False\n",
      "  Off-center particle (r=15) at (32, 40): fits=True\n",
      "  Edge particle (r=15) at (32, 55): fits=False\n",
      "  Max radius at center (32, 32): 30\n",
      "  Max radius at off-center (32, 50): 12\n",
      "  Max radius at extreme position (5, 5): 10.0 (min enforced)\n",
      "\n",
      "  PASSED: Boundary Validation Functions\n"
     ]
    }
   ],
   "source": [
    "def test_boundary_validation():\n",
    "    \"\"\"Test boundary validation to ensure particles fit within grid.\"\"\"\n",
    "    from src.core_shell import validate_particle_bounds, clamp_radius_to_grid\n",
    "    \n",
    "    GRID_SIZE = 64\n",
    "    \n",
    "    # Test 1: Particle that fits\n",
    "    center = (32, 32)  # Center of grid\n",
    "    radius = 20\n",
    "    margin = 2\n",
    "    \n",
    "    result = validate_particle_bounds(radius, center, GRID_SIZE, margin)\n",
    "    assert result == True, f\"Centered particle should fit: radius={radius}, center={center}\"\n",
    "    print(f\"  Centered particle (r={radius}) at {center}: fits={result}\")\n",
    "    \n",
    "    # Test 2: Particle too big to fit\n",
    "    radius_large = 35  # Would extend beyond edges\n",
    "    result = validate_particle_bounds(radius_large, center, GRID_SIZE, margin)\n",
    "    assert result == False, f\"Large particle should NOT fit: radius={radius_large}\"\n",
    "    print(f\"  Large particle (r={radius_large}) at {center}: fits={result}\")\n",
    "    \n",
    "    # Test 3: Off-center particle that still fits\n",
    "    center_off = (32, 40)\n",
    "    radius_small = 15\n",
    "    result = validate_particle_bounds(radius_small, center_off, GRID_SIZE, margin)\n",
    "    assert result == True, f\"Off-center small particle should fit\"\n",
    "    print(f\"  Off-center particle (r={radius_small}) at {center_off}: fits={result}\")\n",
    "    \n",
    "    # Test 4: Off-center particle that doesn't fit\n",
    "    center_edge = (32, 55)\n",
    "    radius_medium = 15\n",
    "    result = validate_particle_bounds(radius_medium, center_edge, GRID_SIZE, margin)\n",
    "    assert result == False, f\"Particle near edge should NOT fit\"\n",
    "    print(f\"  Edge particle (r={radius_medium}) at {center_edge}: fits={result}\")\n",
    "    \n",
    "    # Test 5: clamp_radius_to_grid\n",
    "    center = (32, 32)\n",
    "    max_radius = clamp_radius_to_grid(center, GRID_SIZE, margin)\n",
    "    expected_max = 32 - margin  # 30\n",
    "    assert max_radius == expected_max, f\"Expected max radius {expected_max}, got {max_radius}\"\n",
    "    print(f\"  Max radius at center {center}: {max_radius}\")\n",
    "    \n",
    "    # Test 6: clamp for off-center position\n",
    "    center_off = (32, 50)  # Closer to right edge\n",
    "    max_radius = clamp_radius_to_grid(center_off, GRID_SIZE, margin)\n",
    "    expected_max = GRID_SIZE - margin - 50  # 12\n",
    "    assert max_radius == expected_max, f\"Expected max radius {expected_max}, got {max_radius}\"\n",
    "    print(f\"  Max radius at off-center {center_off}: {max_radius}\")\n",
    "    \n",
    "    # Test 7: Verify minimum radius enforcement\n",
    "    center_extreme = (5, 5)  # Very close to corner\n",
    "    max_radius = clamp_radius_to_grid(center_extreme, GRID_SIZE, margin)\n",
    "    assert max_radius >= 10, f\"Should enforce minimum radius of 10, got {max_radius}\"\n",
    "    print(f\"  Max radius at extreme position {center_extreme}: {max_radius} (min enforced)\")\n",
    "\n",
    "run_test(\"Boundary Validation Functions\", test_boundary_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdx26lmbw2q",
   "metadata": {},
   "source": [
    "---\n",
    "## Test 16: Composition Modes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "vh5uo9f971k",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "TEST: Composition Modes\n",
      "======================================================================\n",
      "  sharp            (Traditional core-shell): Ni=0.94, Fe=0.06\n",
      "  radial_gradient  (Smooth center-to-edge): Ni=0.86, Fe=0.14\n",
      "  linear_gradient  (Linear gradient     ): Ni=0.88, Fe=0.12\n",
      "  janus            (Left-right split    ): Ni=0.88, Fe=0.12\n",
      "  multi_shell      (3-shell onion       ): Ni=0.80, Fe=0.20\n",
      "  uniform          (Uniform Ni3Fe       ): Ni=0.75, Fe=0.25\n",
      "  uniform          (Pure Ni             ): Ni=1.00, Fe=0.00\n",
      "  uniform          (Pure Fe             ): Ni=0.00, Fe=1.00\n",
      "  All composition modes work correctly\n",
      "\n",
      "  PASSED: Composition Modes\n"
     ]
    }
   ],
   "source": [
    "def test_composition_modes():\n",
    "    \"\"\"Test all composition modes: sharp, radial_gradient, linear_gradient, janus, multi_shell, uniform.\"\"\"\n",
    "    from src.core_shell import create_particle_with_shape, SPECIES_NI, SPECIES_FE\n",
    "    \n",
    "    GRID_SIZE = 64\n",
    "    \n",
    "    composition_tests = [\n",
    "        # (mode, params, description, verification_func)\n",
    "        ('sharp', None, 'Traditional core-shell', None),\n",
    "        ('radial_gradient', {'transition_width': 0.3}, 'Smooth center-to-edge', None),\n",
    "        ('linear_gradient', {'gradient_direction': np.pi/4, 'gradient_width': 20.0}, 'Linear gradient', None),\n",
    "        ('janus', {'split_angle': 0.0, 'interface_width': 0.0}, 'Left-right split', None),\n",
    "        ('multi_shell', {'n_shells': 3, 'transition_width': 0.0}, '3-shell onion', None),\n",
    "        ('uniform', {'composition': {'Ni': 0.75, 'Fe': 0.25}}, 'Uniform Ni3Fe', None),\n",
    "        ('uniform', {'composition': {'Ni': 1.0, 'Fe': 0.0}}, 'Pure Ni', None),\n",
    "        ('uniform', {'composition': {'Ni': 0.0, 'Fe': 1.0}}, 'Pure Fe', None),\n",
    "    ]\n",
    "    \n",
    "    for mode, params, desc, _ in composition_tests:\n",
    "        particle, info = create_particle_with_shape(\n",
    "            grid_size=GRID_SIZE,\n",
    "            shape_type='circle',\n",
    "            outer_radius=20,\n",
    "            core_fraction=0.5,\n",
    "            pixel_size=5.0,\n",
    "            composition_mode=mode,\n",
    "            composition_params=params,\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        assert particle.shape == (2, GRID_SIZE, GRID_SIZE), f\"Wrong shape for {mode}\"\n",
    "        \n",
    "        # Get total composition\n",
    "        outer_mask = info['outer_mask']\n",
    "        ni_content = np.abs(particle[SPECIES_NI][outer_mask]).sum()\n",
    "        fe_content = np.abs(particle[SPECIES_FE][outer_mask]).sum()\n",
    "        total = ni_content + fe_content\n",
    "        ni_frac = ni_content / total if total > 0 else 0\n",
    "        fe_frac = fe_content / total if total > 0 else 0\n",
    "        \n",
    "        print(f\"  {mode:16s} ({desc:20s}): Ni={ni_frac:.2f}, Fe={fe_frac:.2f}\")\n",
    "        \n",
    "        # Specific checks for uniform compositions\n",
    "        if mode == 'uniform':\n",
    "            expected_ni = params['composition']['Ni']\n",
    "            expected_fe = params['composition']['Fe']\n",
    "            # Allow some tolerance for averaging effects\n",
    "            assert abs(ni_frac - expected_ni) < 0.05, f\"Ni fraction wrong: {ni_frac} vs {expected_ni}\"\n",
    "            assert abs(fe_frac - expected_fe) < 0.05, f\"Fe fraction wrong: {fe_frac} vs {expected_fe}\"\n",
    "    \n",
    "    print(\"  All composition modes work correctly\")\n",
    "\n",
    "run_test(\"Composition Modes\", test_composition_modes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "i5hxh9lfhdd",
   "metadata": {},
   "source": [
    "---\n",
    "## Test 17: Winterbottom Truncation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "w39iw4zhg6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "TEST: Winterbottom Truncation\n",
      "======================================================================\n",
      "  Base circular mask: 1961 pixels\n",
      "  Truncation=0.10: 116 pixels, removed ~94.08%\n",
      "  Truncation=0.25: 385 pixels, removed ~80.37%\n",
      "  Truncation=0.50: 1006 pixels, removed ~48.70%\n",
      "  Truncation=0.75: 1576 pixels, removed ~19.63%\n",
      "  Angle=0: 518 pixels\n",
      "  Angle=45: 482 pixels\n",
      "  Angle=90: 510 pixels\n",
      "  Angle=180: 503 pixels\n",
      "  Truncated particle created: 470 pixels\n",
      "\n",
      "  PASSED: Winterbottom Truncation\n"
     ]
    }
   ],
   "source": [
    "def test_winterbottom_truncation():\n",
    "    \"\"\"Test Winterbottom truncation at various fractions.\"\"\"\n",
    "    from src.core_shell import apply_winterbottom_truncation, create_particle_with_shape\n",
    "    \n",
    "    GRID_SIZE = 64\n",
    "    center = (GRID_SIZE // 2, GRID_SIZE // 2)\n",
    "    \n",
    "    # Create a circular mask for testing\n",
    "    y, x = np.ogrid[:GRID_SIZE, :GRID_SIZE]\n",
    "    dist_sq = (y - center[0])**2 + (x - center[1])**2\n",
    "    radius = 25\n",
    "    base_mask = dist_sq <= radius**2\n",
    "    base_pixels = base_mask.sum()\n",
    "    print(f\"  Base circular mask: {base_pixels} pixels\")\n",
    "    \n",
    "    # Test various truncation fractions\n",
    "    truncation_tests = [0.1, 0.25, 0.5, 0.75]\n",
    "    \n",
    "    for frac in truncation_tests:\n",
    "        truncated = apply_winterbottom_truncation(\n",
    "            mask=base_mask.copy(),\n",
    "            center=center,\n",
    "            truncation_fraction=frac,\n",
    "            truncation_angle=0.0  # Truncate from bottom\n",
    "        )\n",
    "        \n",
    "        truncated_pixels = truncated.sum()\n",
    "        removed_fraction = 1 - truncated_pixels / base_pixels\n",
    "        \n",
    "        print(f\"  Truncation={frac:.2f}: {truncated_pixels} pixels, removed ~{removed_fraction:.2%}\")\n",
    "        \n",
    "        # Verify truncation removes approximately the expected amount\n",
    "        # (won't be exact due to pixelation, but should be in right ballpark)\n",
    "        assert truncated_pixels < base_pixels, f\"Truncation should remove pixels\"\n",
    "        assert truncated_pixels > 0, f\"Truncation shouldn't remove everything\"\n",
    "    \n",
    "    # Test truncation at different angles\n",
    "    angle_tests = [0, np.pi/4, np.pi/2, np.pi]\n",
    "    for angle in angle_tests:\n",
    "        truncated = apply_winterbottom_truncation(\n",
    "            mask=base_mask.copy(),\n",
    "            center=center,\n",
    "            truncation_fraction=0.3,\n",
    "            truncation_angle=angle\n",
    "        )\n",
    "        print(f\"  Angle={np.degrees(angle):.0f}: {truncated.sum()} pixels\")\n",
    "    \n",
    "    # Test full particle with truncation via create_particle_with_shape\n",
    "    particle, info = create_particle_with_shape(\n",
    "        grid_size=GRID_SIZE,\n",
    "        shape_type='circle',\n",
    "        outer_radius=20,\n",
    "        core_fraction=0.5,\n",
    "        pixel_size=5.0,\n",
    "        truncation_fraction=0.4,\n",
    "        truncation_angle=np.pi/6,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    assert particle.shape == (2, GRID_SIZE, GRID_SIZE), f\"Wrong shape\"\n",
    "    print(f\"  Truncated particle created: {info['outer_mask'].sum()} pixels\")\n",
    "\n",
    "run_test(\"Winterbottom Truncation\", test_winterbottom_truncation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fejieakfj1q",
   "metadata": {},
   "source": [
    "---\n",
    "## Test 18: Off-Center Core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6gj4d9aq5c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "TEST: Off-Center Core\n",
      "======================================================================\n",
      "  Centered core: 489 pixels\n",
      "  Off-center core: 489 pixels\n",
      "  Centered core centroid: (32.0, 32.0)\n",
      "  Off-center core centroid: (37.0, 40.0)\n",
      "  Off-center particle created: core=317 pixels\n",
      "\n",
      "  PASSED: Off-Center Core\n"
     ]
    }
   ],
   "source": [
    "def test_off_center_core():\n",
    "    \"\"\"Test off-center (eccentric) core placement.\"\"\"\n",
    "    from src.core_shell import compute_off_center_core_mask, create_particle_with_shape\n",
    "    \n",
    "    GRID_SIZE = 64\n",
    "    center = (GRID_SIZE // 2, GRID_SIZE // 2)\n",
    "    \n",
    "    # Create outer mask\n",
    "    y, x = np.ogrid[:GRID_SIZE, :GRID_SIZE]\n",
    "    dist_sq = (y - center[0])**2 + (x - center[1])**2\n",
    "    outer_radius = 25\n",
    "    outer_mask = dist_sq <= outer_radius**2\n",
    "    \n",
    "    # Test centered core\n",
    "    core_mask_centered = compute_off_center_core_mask(\n",
    "        outer_mask=outer_mask,\n",
    "        center=center,\n",
    "        core_fraction=0.5,\n",
    "        core_offset=(0.0, 0.0),\n",
    "        shape_type='circle'\n",
    "    )\n",
    "    \n",
    "    # Test off-center core\n",
    "    core_mask_offset = compute_off_center_core_mask(\n",
    "        outer_mask=outer_mask,\n",
    "        center=center,\n",
    "        core_fraction=0.5,\n",
    "        core_offset=(5.0, 8.0),  # Offset by 5 pixels in y, 8 pixels in x\n",
    "        shape_type='circle'\n",
    "    )\n",
    "    \n",
    "    print(f\"  Centered core: {core_mask_centered.sum()} pixels\")\n",
    "    print(f\"  Off-center core: {core_mask_offset.sum()} pixels\")\n",
    "    \n",
    "    # Verify core is contained within outer mask\n",
    "    assert (core_mask_offset & ~outer_mask).sum() == 0, \"Core should be inside outer mask\"\n",
    "    \n",
    "    # Find centroids\n",
    "    def find_centroid(mask):\n",
    "        y_coords, x_coords = np.where(mask)\n",
    "        return np.mean(y_coords), np.mean(x_coords)\n",
    "    \n",
    "    cy_centered, cx_centered = find_centroid(core_mask_centered)\n",
    "    cy_offset, cx_offset = find_centroid(core_mask_offset)\n",
    "    \n",
    "    print(f\"  Centered core centroid: ({cy_centered:.1f}, {cx_centered:.1f})\")\n",
    "    print(f\"  Off-center core centroid: ({cy_offset:.1f}, {cx_offset:.1f})\")\n",
    "    \n",
    "    # The off-center core should have shifted centroid\n",
    "    assert abs(cx_offset - cx_centered) > 3, \"X offset should be visible in centroid\"\n",
    "    assert abs(cy_offset - cy_centered) > 2, \"Y offset should be visible in centroid\"\n",
    "    \n",
    "    # Test full particle with off-center core via create_particle_with_shape\n",
    "    particle, info = create_particle_with_shape(\n",
    "        grid_size=GRID_SIZE,\n",
    "        shape_type='circle',\n",
    "        outer_radius=20,\n",
    "        core_fraction=0.5,\n",
    "        pixel_size=5.0,\n",
    "        core_offset=(3.0, 4.0),\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    assert particle.shape == (2, GRID_SIZE, GRID_SIZE), f\"Wrong shape\"\n",
    "    print(f\"  Off-center particle created: core={info['core_mask'].sum()} pixels\")\n",
    "\n",
    "run_test(\"Off-Center Core\", test_off_center_core)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aie1du0ssk5",
   "metadata": {},
   "source": [
    "---\n",
    "## Test 19: Size Variation with Boundary Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "jjfhkjgcr",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "TEST: Size Variation with Boundary Checking\n",
      "======================================================================\n",
      "  Scale=0.70, r=28: 2453 pixels, edge_pixels=0\n",
      "  Scale=0.85, r=34: 3625 pixels, edge_pixels=0\n",
      "  Scale=1.00, r=40: 5025 pixels, edge_pixels=0\n",
      "  Scale=1.15, r=46: 6625 pixels, edge_pixels=0\n",
      "  Scale=1.30, r=52: 8497 pixels, edge_pixels=0\n",
      "  Boundary check correctly rejects r=67\n",
      "\n",
      "  PASSED: Size Variation with Boundary Checking\n"
     ]
    }
   ],
   "source": [
    "def test_size_variation():\n",
    "    \"\"\"Test particles at various sizes and verify they fit in grid.\"\"\"\n",
    "    from src.core_shell import create_particle_with_shape, validate_particle_bounds\n",
    "    \n",
    "    GRID_SIZE = 128  # Standard size\n",
    "    center = (GRID_SIZE // 2, GRID_SIZE // 2)\n",
    "    \n",
    "    # Test various radius sizes (simulating scale factors from 0.7 to 1.3)\n",
    "    base_radius = 40\n",
    "    scale_factors = [0.7, 0.85, 1.0, 1.15, 1.3]\n",
    "    \n",
    "    for scale in scale_factors:\n",
    "        outer_radius = int(base_radius * scale)\n",
    "        \n",
    "        # Verify it fits before creating\n",
    "        fits = validate_particle_bounds(outer_radius, center, GRID_SIZE, margin=2)\n",
    "        \n",
    "        if fits:\n",
    "            particle, info = create_particle_with_shape(\n",
    "                grid_size=GRID_SIZE,\n",
    "                shape_type='circle',\n",
    "                outer_radius=outer_radius,\n",
    "                core_fraction=0.5,\n",
    "                pixel_size=5.0,\n",
    "                verbose=False\n",
    "            )\n",
    "            \n",
    "            # Verify particle stays within bounds\n",
    "            outer_mask = info['outer_mask']\n",
    "            \n",
    "            # Check no pixels at edges\n",
    "            edge_top = outer_mask[0, :].sum()\n",
    "            edge_bottom = outer_mask[-1, :].sum()\n",
    "            edge_left = outer_mask[:, 0].sum()\n",
    "            edge_right = outer_mask[:, -1].sum()\n",
    "            \n",
    "            total_edge = edge_top + edge_bottom + edge_left + edge_right\n",
    "            \n",
    "            print(f\"  Scale={scale:.2f}, r={outer_radius}: {outer_mask.sum()} pixels, edge_pixels={total_edge}\")\n",
    "            \n",
    "            # With margin=2, should have no pixels at the very edges\n",
    "            assert total_edge == 0, f\"Particle touches edges at scale={scale}\"\n",
    "        else:\n",
    "            print(f\"  Scale={scale:.2f}, r={outer_radius}: WOULD NOT FIT (boundary check caught it)\")\n",
    "    \n",
    "    # Test that boundary check catches too-large particles\n",
    "    max_possible = GRID_SIZE // 2 - 2  # 62 for 128 grid\n",
    "    result = validate_particle_bounds(max_possible + 5, center, GRID_SIZE, margin=2)\n",
    "    assert result == False, \"Should reject particle larger than grid allows\"\n",
    "    print(f\"  Boundary check correctly rejects r={max_possible + 5}\")\n",
    "\n",
    "run_test(\"Size Variation with Boundary Checking\", test_size_variation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vm9576r2sla",
   "metadata": {},
   "source": [
    "---\n",
    "## Test 20: Backwards Compatibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0n9mbn7jghee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "TEST: Backwards Compatibility\n",
      "======================================================================\n",
      "  Default hexagon particle created\n",
      "  Core: Ni=52.5, Fe=17.5 (expected ~75% Ni, ~25% Fe)\n",
      "  Shell: Ni=210.0, Fe=0.0 (expected ~100% Ni)\n",
      "  Default core-shell composition verified (Ni3Fe core, pure Ni shell)\n",
      "  circle: OK\n",
      "  polygon: OK\n",
      "  polygon_centrosymmetric: OK\n",
      "\n",
      "  PASSED: Backwards Compatibility\n"
     ]
    }
   ],
   "source": [
    "def test_backwards_compatibility():\n",
    "    \"\"\"Test that old-style calls without new parameters still work.\"\"\"\n",
    "    from src.core_shell import create_particle_with_shape, SPECIES_NI, SPECIES_FE\n",
    "    \n",
    "    GRID_SIZE = 64\n",
    "    \n",
    "    # Original call style (no composition_mode, no truncation, no offset)\n",
    "    particle, info = create_particle_with_shape(\n",
    "        grid_size=GRID_SIZE,\n",
    "        shape_type='hexagon',\n",
    "        outer_radius=20,\n",
    "        core_fraction=0.5,\n",
    "        pixel_size=5.0,\n",
    "        shape_params={'anisotropy': 1.2},\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    assert particle.shape == (2, GRID_SIZE, GRID_SIZE), f\"Wrong shape\"\n",
    "    assert 'outer_mask' in info, \"Missing outer_mask\"\n",
    "    assert 'core_mask' in info, \"Missing core_mask\"\n",
    "    assert 'shell_mask' in info, \"Missing shell_mask\"\n",
    "    \n",
    "    # Verify core-shell composition (default behavior)\n",
    "    # Default composition: Core = Ni3Fe (75% Ni, 25% Fe), Shell = pure Ni (100% Ni)\n",
    "    core_mask = info['core_mask']\n",
    "    shell_mask = info['shell_mask']\n",
    "    \n",
    "    fe_in_core = np.abs(particle[SPECIES_FE][core_mask]).sum()\n",
    "    ni_in_core = np.abs(particle[SPECIES_NI][core_mask]).sum()\n",
    "    \n",
    "    fe_in_shell = np.abs(particle[SPECIES_FE][shell_mask]).sum()\n",
    "    ni_in_shell = np.abs(particle[SPECIES_NI][shell_mask]).sum()\n",
    "    \n",
    "    print(f\"  Default hexagon particle created\")\n",
    "    print(f\"  Core: Ni={ni_in_core:.1f}, Fe={fe_in_core:.1f} (expected ~75% Ni, ~25% Fe)\")\n",
    "    print(f\"  Shell: Ni={ni_in_shell:.1f}, Fe={fe_in_shell:.1f} (expected ~100% Ni)\")\n",
    "    \n",
    "    # Core is Ni3Fe (75% Ni, 25% Fe) - Ni-rich, with some Fe\n",
    "    # Check that core has both Ni and Fe, with Ni dominant\n",
    "    assert ni_in_core > fe_in_core, \"Core should be Ni-rich (Ni3Fe = 75% Ni)\"\n",
    "    assert fe_in_core > 0, \"Core should contain some Fe (Ni3Fe = 25% Fe)\"\n",
    "    \n",
    "    # Shell should be nearly pure Ni (no Fe)\n",
    "    assert ni_in_shell > 0, \"Shell should contain Ni\"\n",
    "    assert fe_in_shell < ni_in_shell * 0.1, \"Shell should be nearly pure Ni (< 10% Fe)\"\n",
    "    \n",
    "    print(\"  Default core-shell composition verified (Ni3Fe core, pure Ni shell)\")\n",
    "    \n",
    "    # Test that other old-style shapes still work\n",
    "    for shape in ['circle', 'polygon', 'polygon_centrosymmetric']:\n",
    "        params = {'n_vertices': 5} if 'polygon' in shape else {}\n",
    "        p, i = create_particle_with_shape(\n",
    "            grid_size=GRID_SIZE,\n",
    "            shape_type=shape,\n",
    "            outer_radius=18,\n",
    "            core_fraction=0.5,\n",
    "            pixel_size=5.0,\n",
    "            shape_params=params,\n",
    "            verbose=False\n",
    "        )\n",
    "        assert p.shape == (2, GRID_SIZE, GRID_SIZE), f\"Shape {shape} failed\"\n",
    "        print(f\"  {shape}: OK\")\n",
    "\n",
    "run_test(\"Backwards Compatibility\", test_backwards_compatibility)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "azlaxllal1f",
   "metadata": {},
   "source": [
    "---\n",
    "## Test 21: Training Data Generation with New Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "yb7chgpezl",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "TEST: Training Data Generation with New Features\n",
      "======================================================================\n",
      "Loading scattering factors for Ni from ../data/Nickel.f1f2...\n",
      "  Loaded 4601 data points\n",
      "  Energy range: 2000.0 - 25000.0 eV\n",
      "Loading scattering factors for Fe from ../data/Iron.f1f2...\n",
      "  Loaded 4601 data points\n",
      "  Energy range: 2000.0 - 25000.0 eV\n",
      "  SHAPE_DISTRIBUTION: ['hexagon', 'polygon', 'polygon_centrosymmetric', 'circle', 'ellipse']\n",
      "  COMPOSITION_DISTRIBUTION: ['sharp', 'radial_gradient', 'linear_gradient', 'janus', 'multi_shell', 'uniform']\n",
      "  Sampled shapes (100 trials): {'hexagon': 35, 'polygon': 21, 'circle': 17, 'polygon_centrosymmetric': 13, 'ellipse': 14}\n",
      "  Sampled modes (100 trials): {'linear_gradient': 16, 'multi_shell': 4, 'sharp': 42, 'janus': 7, 'radial_gradient': 11, 'uniform': 20}\n",
      "  sharp: Generated 64 patches\n",
      "  radial_gradient: Generated 64 patches\n",
      "  janus: Generated 64 patches\n",
      "  uniform: Generated 64 patches\n",
      "  Truncation sampled in 50 trials: True\n",
      "  Off-center core sampled in 50 trials: True\n",
      "\n",
      "  PASSED: Training Data Generation with New Features\n"
     ]
    }
   ],
   "source": [
    "def test_training_data_new_features():\n",
    "    \"\"\"Test training data generation with new composition modes and geometry features.\"\"\"\n",
    "    from src.generate_data import (\n",
    "        sample_shape_type, sample_parameters, sample_composition_mode,\n",
    "        sample_composition_parameters, process_particle,\n",
    "        SHAPE_DISTRIBUTION, COMPOSITION_DISTRIBUTION\n",
    "    )\n",
    "    from src.core_shell import ScatteringFactors\n",
    "    \n",
    "    sf = ScatteringFactors()  # Auto-detect data directory\n",
    "    \n",
    "    # Test 1: Verify distributions are defined\n",
    "    print(f\"  SHAPE_DISTRIBUTION: {list(SHAPE_DISTRIBUTION.keys())}\")\n",
    "    assert 'ellipse' in SHAPE_DISTRIBUTION, \"Missing ellipse shape\"\n",
    "    assert sum(SHAPE_DISTRIBUTION.values()) > 0.99, \"Shape distribution should sum to ~1\"\n",
    "    \n",
    "    print(f\"  COMPOSITION_DISTRIBUTION: {list(COMPOSITION_DISTRIBUTION.keys())}\")\n",
    "    assert 'uniform' in COMPOSITION_DISTRIBUTION, \"Missing uniform mode\"\n",
    "    assert 'radial_gradient' in COMPOSITION_DISTRIBUTION, \"Missing radial_gradient mode\"\n",
    "    assert sum(COMPOSITION_DISTRIBUTION.values()) > 0.99, \"Composition distribution should sum to ~1\"\n",
    "    \n",
    "    # Test 2: Sample various shape types\n",
    "    shape_counts = {}\n",
    "    for _ in range(100):\n",
    "        shape = sample_shape_type()\n",
    "        shape_counts[shape] = shape_counts.get(shape, 0) + 1\n",
    "    print(f\"  Sampled shapes (100 trials): {shape_counts}\")\n",
    "    \n",
    "    # Test 3: Sample various composition modes\n",
    "    mode_counts = {}\n",
    "    for _ in range(100):\n",
    "        mode = sample_composition_mode()\n",
    "        mode_counts[mode] = mode_counts.get(mode, 0) + 1\n",
    "    print(f\"  Sampled modes (100 trials): {mode_counts}\")\n",
    "    \n",
    "    # Test 4: Generate particles with specific composition modes\n",
    "    test_modes = ['sharp', 'radial_gradient', 'janus', 'uniform']\n",
    "    for mode in test_modes:\n",
    "        shape = sample_shape_type()\n",
    "        params = sample_parameters(shape, mode)\n",
    "        \n",
    "        # Verify params contain new fields\n",
    "        assert 'composition_mode' in params, f\"Missing composition_mode in params for {mode}\"\n",
    "        assert params['composition_mode'] == mode, f\"Wrong mode: {params['composition_mode']}\"\n",
    "        \n",
    "        # Generate the particle\n",
    "        data = process_particle(params, sf, verbose=False)\n",
    "        assert data is not None, f\"process_particle failed for mode={mode}\"\n",
    "        print(f\"  {mode}: Generated {data['n_patches']} patches\")\n",
    "    \n",
    "    # Test 5: Verify truncation and off-center are sometimes sampled\n",
    "    has_truncation = False\n",
    "    has_offset = False\n",
    "    for _ in range(50):\n",
    "        shape = sample_shape_type()\n",
    "        mode = sample_composition_mode()\n",
    "        params = sample_parameters(shape, mode)\n",
    "        if params.get('truncation_fraction', 0) > 0:\n",
    "            has_truncation = True\n",
    "        if params.get('core_offset', (0, 0)) != (0, 0):\n",
    "            has_offset = True\n",
    "    \n",
    "    print(f\"  Truncation sampled in 50 trials: {has_truncation}\")\n",
    "    print(f\"  Off-center core sampled in 50 trials: {has_offset}\")\n",
    "\n",
    "run_test(\"Training Data Generation with New Features\", test_training_data_new_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xxu9nbx7lj",
   "metadata": {},
   "source": [
    "---\n",
    "## Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "oexrj7hjyo",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TEST RESULTS SUMMARY\n",
      "================================================================================\n",
      "  [PASS] Core Module Imports\n",
      "  [SKIP] PyTorch Module Imports\n",
      "  [PASS] Scattering Factors\n",
      "  [PASS] Particle Creation (All Types)\n",
      "  [PASS] Strain Field Generation\n",
      "  [PASS] Diffraction Computation\n",
      "  [PASS] Ground Truth Labels\n",
      "  [SKIP] CNN Architecture\n",
      "  [SKIP] Loss Function\n",
      "  [PASS] Data Augmentation\n",
      "  [PASS] MAD Equation Consistency\n",
      "  [PASS] Training Data Generation\n",
      "  [SKIP] Dataset Loading\n",
      "  [PASS] Ellipse Shape Creation\n",
      "  [PASS] Boundary Validation Functions\n",
      "  [PASS] Composition Modes\n",
      "  [PASS] Winterbottom Truncation\n",
      "  [PASS] Off-Center Core\n",
      "  [PASS] Size Variation with Boundary Checking\n",
      "  [PASS] Backwards Compatibility\n",
      "  [PASS] Training Data Generation with New Features\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "TOTAL: 17 passed, 0 failed, 4 skipped out of 21 tests\n",
      "================================================================================\n",
      "\n",
      "ALL RUNNABLE TESTS PASSED! (4 skipped due to missing PyTorch)\n"
     ]
    }
   ],
   "source": [
    "# Print summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TEST RESULTS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "passed = sum(1 for _, status, _ in test_results if status == 'PASSED')\n",
    "failed = sum(1 for _, status, _ in test_results if status == 'FAILED')\n",
    "skipped = sum(1 for _, status, _ in test_results if status == 'SKIPPED')\n",
    "\n",
    "for name, status, error in test_results:\n",
    "    if status == 'PASSED':\n",
    "        symbol = \"PASS\"\n",
    "    elif status == 'SKIPPED':\n",
    "        symbol = \"SKIP\"\n",
    "    else:\n",
    "        symbol = \"FAIL\"\n",
    "    print(f\"  [{symbol}] {name}\")\n",
    "    if error and status == 'FAILED':\n",
    "        # Show more of the error message\n",
    "        error_display = error[:120] + \"...\" if len(error) > 120 else error\n",
    "        print(f\"         Error: {error_display}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(f\"TOTAL: {passed} passed, {failed} failed, {skipped} skipped out of {len(test_results)} tests\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if failed == 0:\n",
    "    if skipped > 0:\n",
    "        print(f\"\\nALL RUNNABLE TESTS PASSED! ({skipped} skipped due to missing PyTorch)\")\n",
    "    else:\n",
    "        print(\"\\nALL TESTS PASSED!\")\n",
    "else:\n",
    "    print(f\"\\n{failed} TEST(S) FAILED - see details above\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
